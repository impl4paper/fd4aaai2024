{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "baad6458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset, Subset, random_split\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "from torchvision.datasets import MNIST, EMNIST\n",
    "import torch.nn.functional as F\n",
    "from matplotlib.pyplot import subplots\n",
    "from torchvision.utils import save_image\n",
    "import torch.optim as optim\n",
    "from tensorboardX import SummaryWriter\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "bs = 100\n",
    "n_epoch = 50\n",
    "dim=100\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "writer = SummaryWriter(os.path.join('../lra', 'beta'))\n",
    "gpu = 1\n",
    "device = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7fa70b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_inference(net, testloader):\n",
    "    \"\"\" Returns the test accuracy and loss.\n",
    "    \"\"\"\n",
    "    net.eval()\n",
    "    loss, total, correct = 0.0, 0.0, 0.0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    criterion.cuda(gpu)\n",
    "    mnist_dim = train_dataset.train_data.size(1) * train_dataset.train_data.size(2)\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, labels) in enumerate(testloader):\n",
    "            images, labels = images.cuda(gpu), labels.cuda(gpu)\n",
    "            \n",
    "            # Inference\n",
    "            outputs = net(images)\n",
    "            batch_loss = criterion(outputs, labels)\n",
    "            loss += copy.deepcopy(batch_loss.item())\n",
    "\n",
    "            # Prediction\n",
    "            _, pred_labels = torch.max(outputs, 1)\n",
    "            pred_labels = pred_labels.view(-1)\n",
    "            correct += torch.sum(torch.eq(pred_labels, labels)).item()\n",
    "            total += len(labels)\n",
    "    accuracy = correct/total\n",
    "    return accuracy, loss\n",
    "\n",
    "def test_error(model, steal_model, testloader):\n",
    "    model.eval()\n",
    "    steal_model.eval()\n",
    "    loss, total, correct = 0.0, 0.0, 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, labels) in enumerate(testloader):\n",
    "            images, labels = images.cuda(gpu), labels.cuda(gpu)\n",
    "\n",
    "            # Inference\n",
    "            outputs0 = model(images)\n",
    "            outputs1 = steal_model(images)\n",
    "            \n",
    "            # Prediction\n",
    "            _, pred_labels0 = torch.max(outputs0, 1)\n",
    "            pred_labels0 = pred_labels0.view(-1)\n",
    "            \n",
    "            _, pred_labels1 = torch.max(outputs1, 1)\n",
    "            pred_labels1 = pred_labels1.view(-1)\n",
    "            \n",
    "            correct += torch.sum(torch.eq(pred_labels0, pred_labels1)).item()\n",
    "            total += len(labels)\n",
    "    accuracy = correct/total\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def stat_num(model, steal_model, testloader):\n",
    "    model.eval()\n",
    "    steal_model.eval()\n",
    "    loss, total, correct = 0.0, 0.0, 0.0\n",
    "    l2n = {i: 0 for i in range(11)}\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, labels) in enumerate(testloader):\n",
    "            images, labels = images.cuda(gpu), labels.cuda(gpu)\n",
    "\n",
    "            # Inference\n",
    "            outputs0 = model(images)\n",
    "            outputs1 = steal_model(images)\n",
    "            \n",
    "            # Prediction\n",
    "            _, pred_labels0 = torch.max(outputs0, 1)\n",
    "            pred_labels0 = pred_labels0.view(-1)\n",
    "            \n",
    "            _, pred_labels1 = torch.max(outputs1, 1)\n",
    "            pred_labels1 = pred_labels1.view(-1)\n",
    "            \n",
    "            list0 = pred_labels0.cpu().numpy().tolist()\n",
    "            list1 = pred_labels1.cpu().numpy().tolist()\n",
    "            for i in range(len(list0)):\n",
    "                if list0[i] == list1[i]:\n",
    "                    l2n[list0[i]] += 1\n",
    "    for k, v in l2n.items():\n",
    "        print(f\"Label {k}: {v}\")\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "        Convolutional Discriminator for MNIST\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channel=1, num_classes=11):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            # 28 -> 14\n",
    "            nn.Conv2d(in_channel, 512, 3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # 14 -> 7\n",
    "            nn.Conv2d(512, 256, 3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # 7 -> 4\n",
    "            nn.Conv2d(256, 128, 3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.AvgPool2d(4),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            # reshape input, 128 -> 1\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, y=None):\n",
    "        y_ = self.conv(x)\n",
    "        y_ = y_.view(y_.size(0), -1)\n",
    "        y_ = self.fc(y_)\n",
    "        return y_\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "        Convolutional Generator for MNIST\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size=100, num_classes=784):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size, 4*4*512),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv = nn.Sequential(\n",
    "            # input: 4 by 4, output: 7 by 7\n",
    "            nn.ConvTranspose2d(512, 256, 3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            # input: 7 by 7, output: 14 by 14\n",
    "            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            # input: 14 by 14, output: 28 by 28\n",
    "            nn.ConvTranspose2d(128, 1, 4, stride=2, padding=1, bias=False),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, y=None):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        y_ = self.fc(x)\n",
    "        y_ = y_.view(y_.size(0), 512, 4, 4)\n",
    "        y_ = self.conv(y_)\n",
    "        return y_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "960be3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5,), std=(0.5,))])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='../data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='../data', train=False, transform=transform, download=False)\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=bs, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9249b24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xdy/anaconda3/lib/python3.7/site-packages/torchvision/datasets/mnist.py:52: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "labels = train_dataset.train_labels\n",
    "indices0,indices1 = [], []\n",
    "for i in range(0, 10):\n",
    "    indices0 += (labels == i).nonzero().view(-1).tolist()\n",
    "for i in range(5, 10):\n",
    "    indices1 += (labels == i).nonzero().view(-1).tolist()\n",
    "localset0, localset1 = Subset(train_dataset, indices0), Subset(train_dataset, indices1)\n",
    "dataloader0, dataloader1 =DataLoader(localset0, batch_size=bs,shuffle=True), DataLoader(localset1, batch_size=bs,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7632a25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xdy/anaconda3/lib/python3.7/site-packages/torchvision/datasets/mnist.py:62: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n"
     ]
    }
   ],
   "source": [
    "z_dim = 100\n",
    "mnist_dim = train_dataset.train_data.size(1) * train_dataset.train_data.size(2)\n",
    "G = Generator().to(gpu)\n",
    "D = Discriminator().to(gpu)\n",
    "criterion = nn.CrossEntropyLoss().to(gpu)\n",
    "kl_criterion = nn.KLDivLoss(reduction='batchmean').to(gpu)\n",
    "# optimizer\n",
    "lr = 0.0002 \n",
    "G_optimizer = optim.Adam(G.parameters(), lr = lr)\n",
    "D_optimizer = optim.Adam(D.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d981589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zoo(D, x_fake, y_fake, D_optimizer):\n",
    "    grad_est = torch.zeros_like(x_fake).to(gpu)\n",
    "    #reduction='none'\n",
    "    criterion = nn.CrossEntropyLoss().cuda(gpu)\n",
    "    kl_criterion = nn.KLDivLoss(reduction='batchmean').to(gpu)\n",
    "    m = 20\n",
    "    epsilon = 0.1\n",
    "    N = x_fake.size(0)\n",
    "    C = x_fake.size(1)\n",
    "    S = x_fake.size(2)\n",
    "    d = S**2 * C\n",
    "    \n",
    "    lossG_target = criterion(D(torch.tanh(x_fake)),y_fake)\n",
    "    \n",
    "    for i in range(m):\n",
    "        u = torch.randn(x_fake.size()).cuda(gpu)\n",
    "        u_flat = u.view([N, -1])\n",
    "        u_norm = u / torch.norm(u_flat, dim=1).view([-1, 1, 1, 1])\n",
    "        x_mod_pre = x_fake + (epsilon * u_norm)\n",
    "        Tout = D(torch.tanh(x_mod_pre))\n",
    "        \n",
    "        lossG_target_mod = criterion(Tout, y_fake)\n",
    "        grad_est += (\n",
    "                (d / m) * (lossG_target_mod - lossG_target) / epsilon\n",
    "            ).view([-1, 1, 1, 1]) * u_norm\n",
    "        \n",
    "    grad_est /= N\n",
    "    #return grad_est\n",
    "    \n",
    "    D.zero_grad()\n",
    "    x_det_pre = x_fake.detach()\n",
    "    Tout = D(torch.tanh(x_det_pre))\n",
    "    \n",
    "    fake_label = torch.topk(Tout.detach(), 1)[1].squeeze(1)\n",
    "    lossG_det = criterion(Tout, fake_label)\n",
    "    lossG_det.backward()\n",
    "    D_optimizer.step()\n",
    "    return lossG.mean(), 0, 0, grad_est\n",
    "    \n",
    "    \n",
    "#     lossG_det = kl_criterion(F.log_softmax(Tout, dim=1), F.log_softmax(Variable(Tout), dim=1))\n",
    "#     print(f\"lossG_det: {lossG_det.item()}\")\n",
    "   \n",
    "#     lossG_det.backward()\n",
    "#     for name, param in D.named_parameters():\n",
    "#         param_gard = param.grad.data\n",
    "#         print(param_gard)\n",
    "#     return lossG.mean(), 0, 0, grad_est\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     lossG_det = criterion(Tout, y_fake)\n",
    "#     lossG_det.backward()\n",
    "#     grad_true_flat = x_det_pre.grad.view([N, -1])\n",
    "\n",
    "#     cos = nn.CosineSimilarity(dim=1)\n",
    "#     cs = cos(grad_true_flat, grad_est_flat)\n",
    "#     mag_ratio = grad_est_flat.norm(2, dim=1) / grad_true_flat.norm(2, dim=1)\n",
    "#     lossG = lossG_det.detach()\n",
    "#     return lossG.mean(), cs.mean(), mag_ratio.mean(), grad_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ceac2313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Local Epoch: 0: Loss: 0.32365119457244873, Acc: 0.942\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'lossG' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2404/2283479248.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mG_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mtemp_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mloss_g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmag_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimate_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzoo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"loss_g: {loss_g.item()} | cs: {cs.item()} | mag_ratio: {mag_ratio.item()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mG_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimate_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2404/3908105127.py\u001b[0m in \u001b[0;36mzoo\u001b[0;34m(D, x_fake, y_fake, D_optimizer)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mlossG_det\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mD_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlossG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_est\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lossG' is not defined"
     ]
    }
   ],
   "source": [
    "class FakeData(Dataset):\n",
    "    def __init__(self):\n",
    "        super(FakeData, self).__init__()\n",
    "        count = 25000\n",
    "        self.x = []\n",
    "        self.y = []\n",
    "        for i in range(count):\n",
    "            self.x.append(Variable(torch.randn(z_dim)))\n",
    "            self.y.append(Variable(torch.zeros(1).fill_(10).long()))\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "\n",
    "epochs = 400\n",
    "local_ep = 1\n",
    "D.train()\n",
    "G.train()\n",
    "for epoch in range(epochs):\n",
    "    for e in range(local_ep):\n",
    "        for pri_x, pri_y in dataloader0:\n",
    "            pri_x, pri_y = pri_x.to(gpu), pri_y.to(gpu)\n",
    "            D.zero_grad()\n",
    "            pri_logit = D(pri_x)\n",
    "            loss = criterion(pri_logit, pri_y)\n",
    "            loss.backward()\n",
    "            D_optimizer.step()\n",
    "    acc, _ = test_inference(D, test_loader)\n",
    "    print(f\"Epoch: {epoch} | Local Epoch: {e}: Loss: {loss.item()}, Acc: {acc}\")\n",
    "    \n",
    "    # distributs public dataset with some fake data\n",
    "    fakedataset = FakeData()\n",
    "    fakedataloader = DataLoader(fakedataset, batch_size=bs,shuffle=False)\n",
    "    \n",
    "    for fake_x, fake_y in fakedataloader:\n",
    "        fake_x, fake_y = fake_x.to(gpu), fake_y.view(-1).to(gpu)\n",
    "        G.zero_grad()\n",
    "        G_output = G(fake_x)\n",
    "        temp_fake = copy.deepcopy(fake_y).fill_(0)\n",
    "        loss_g, cs, mag_ratio, estimate_grad = zoo(D, G_output, temp_fake, D_optimizer)\n",
    "        print(f\"loss_g: {loss_g.item()} | cs: {cs.item()} | mag_ratio: {mag_ratio.item()}\")\n",
    "        G_output.backward(estimate_grad)\n",
    "        G_optimizer.step()\n",
    "    for fake_x, fake_y in fakedataloader:\n",
    "        fake_x, fake_y = fake_x.to(gpu), fake_y.view(-1).to(gpu)\n",
    "        D.zero_grad()\n",
    "        G_output = G(fake_x)\n",
    "        pub_logit = D(G_output)\n",
    "        loss_d = criterion(pub_logit, fake_y)\n",
    "        loss_d.backward()\n",
    "        D_optimizer.step()\n",
    "    with torch.no_grad():\n",
    "        test_z = Variable(torch.randn(50, z_dim).to(device))\n",
    "        generated = G(test_z)\n",
    "        #out0grid = torchvision.utils.make_grid(generated.view(generated.size(0), 1, 28, 28), nrow=50)\n",
    "        out0grid = torchvision.utils.make_grid(generated, nrow=50)\n",
    "        writer.add_image('images', out0grid, epoch)\n",
    "        \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18715af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
