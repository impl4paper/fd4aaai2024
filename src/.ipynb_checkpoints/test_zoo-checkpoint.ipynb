{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "348fdf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset, Subset, random_split,RandomSampler\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.autograd import Variable\n",
    "from torchvision.datasets import MNIST, EMNIST\n",
    "import torch.nn.functional as F\n",
    "from matplotlib.pyplot import subplots\n",
    "from torchvision.utils import save_image\n",
    "import torch.optim as optim\n",
    "from tensorboardX import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "bs = 128\n",
    "n_epoch = 50\n",
    "dim=100\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "gpu = 1\n",
    "device = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34c9f4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_inference(net, testloader):\n",
    "    \"\"\" Returns the test accuracy and loss.\n",
    "    \"\"\"\n",
    "    net.eval()\n",
    "    loss, total, correct = 0.0, 0.0, 0.0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    criterion.cuda(gpu)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, labels) in enumerate(testloader):\n",
    "            images, labels = images.cuda(gpu), labels.cuda(gpu)\n",
    "            \n",
    "            # Inference\n",
    "            outputs = net(images).squeeze()\n",
    "            batch_loss = criterion(outputs, labels)\n",
    "            loss += copy.deepcopy(batch_loss.item())\n",
    "\n",
    "            # Prediction\n",
    "            _, pred_labels = torch.max(outputs, 1)\n",
    "            pred_labels = pred_labels.view(-1)\n",
    "            correct += torch.sum(torch.eq(pred_labels, labels)).item()\n",
    "            total += len(labels)\n",
    "    accuracy = correct/total\n",
    "    return accuracy, loss\n",
    "\n",
    "class generator(nn.Module):\n",
    "    # initializers\n",
    "    def __init__(self, d=128):\n",
    "        super(generator, self).__init__()\n",
    "        self.deconv1 = nn.ConvTranspose2d(100, d*8, 4, 1, 0)\n",
    "        self.deconv1_bn = nn.BatchNorm2d(d*8)\n",
    "        self.deconv2 = nn.ConvTranspose2d(d*8, d*4, 4, 2, 1)\n",
    "        self.deconv2_bn = nn.BatchNorm2d(d*4)\n",
    "        self.deconv3 = nn.ConvTranspose2d(d*4, d*2, 4, 2, 1)\n",
    "        self.deconv3_bn = nn.BatchNorm2d(d*2)\n",
    "        self.deconv4 = nn.ConvTranspose2d(d*2, d, 4, 2, 1)\n",
    "        self.deconv4_bn = nn.BatchNorm2d(d)\n",
    "        self.deconv5 = nn.ConvTranspose2d(d, 3, 4, 2, 1)\n",
    "\n",
    "    # weight_init\n",
    "    def weight_init(self, mean, std):\n",
    "        for m in self._modules:\n",
    "            normal_init(self._modules[m], mean, std)\n",
    "\n",
    "    # forward method\n",
    "    def forward(self, input):\n",
    "        # x = F.relu(self.deconv1(input))\n",
    "        x = F.relu(self.deconv1_bn(self.deconv1(input)))\n",
    "        x = F.relu(self.deconv2_bn(self.deconv2(x)))\n",
    "        x = F.relu(self.deconv3_bn(self.deconv3(x)))\n",
    "        x = F.relu(self.deconv4_bn(self.deconv4(x)))\n",
    "        x = F.tanh(self.deconv5(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "class discriminator(nn.Module):\n",
    "    # initializers\n",
    "    def __init__(self, d=128):\n",
    "        super(discriminator, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, d, 4, 2, 1)\n",
    "        self.conv2 = nn.Conv2d(d, d*2, 4, 2, 1)\n",
    "        self.conv2_bn = nn.BatchNorm2d(d*2)\n",
    "        self.conv3 = nn.Conv2d(d*2, d*4, 4, 2, 1)\n",
    "        self.conv3_bn = nn.BatchNorm2d(d*4)\n",
    "        self.conv4 = nn.Conv2d(d*4, d*8, 4, 2, 1)\n",
    "        self.conv4_bn = nn.BatchNorm2d(d*8)\n",
    "        self.conv5 = nn.Conv2d(d*8, 5, 4, 1, 0)# 1-6\n",
    "        #self.linear = nn.Linear(16384, 6)\n",
    "\n",
    "    # weight_init\n",
    "    def weight_init(self, mean, std):\n",
    "        for m in self._modules:\n",
    "            normal_init(self._modules[m], mean, std)\n",
    "\n",
    "    # forward method\n",
    "    def forward(self, input):\n",
    "        x = F.leaky_relu(self.conv1(input), 0.2)\n",
    "        x = F.leaky_relu(self.conv2_bn(self.conv2(x)), 0.2)\n",
    "        x = F.leaky_relu(self.conv3_bn(self.conv3(x)), 0.2)\n",
    "        x = F.leaky_relu(self.conv4_bn(self.conv4(x)), 0.2)\n",
    "        #print(x.size())\n",
    "        x = self.conv5(x)\n",
    "        #print(x.size())\n",
    "        #x = F.sigmoid(self.conv5(x))\n",
    "        #x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "def normal_init(m, mean, std):\n",
    "    if isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Conv2d):\n",
    "        m.weight.data.normal_(mean, std)\n",
    "        m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70b1060a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g_loss: 13.510198593139648, d_loss: 0.055531296879053116\n",
      "g_loss: 13.542549133300781, d_loss: 0.08645309507846832\n",
      "g_loss: 13.635666847229004, d_loss: 0.043263014405965805\n",
      "g_loss: 19.18427085876465, d_loss: 0.04111462086439133\n",
      "g_loss: 18.022476196289062, d_loss: 0.06549662351608276\n",
      "g_loss: 18.800886154174805, d_loss: 0.0463365763425827\n",
      "g_loss: 31.526561737060547, d_loss: 0.02266242355108261\n",
      "g_loss: 36.52996826171875, d_loss: 0.013080516830086708\n",
      "g_loss: 20.804784774780273, d_loss: 0.015114218927919865\n",
      "g_loss: 34.2892951965332, d_loss: 0.046089306473731995\n",
      "g_loss: 25.4948787689209, d_loss: 0.013570869341492653\n",
      "g_loss: 51.12068557739258, d_loss: 0.009643634781241417\n",
      "g_loss: 21.865283966064453, d_loss: 0.013577502220869064\n",
      "g_loss: 24.053010940551758, d_loss: 0.008960557170212269\n",
      "g_loss: 30.558578491210938, d_loss: 0.0022932521533221006\n",
      "g_loss: 51.90040969848633, d_loss: 0.010763954371213913\n",
      "g_loss: 24.694761276245117, d_loss: 0.00936624500900507\n",
      "g_loss: 22.159046173095703, d_loss: 0.015126162208616734\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27374/2686628574.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mx_mod_pre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfake_x\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mu_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mx_mod_pre_tanh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_mod_pre\u001b[0m \u001b[0;31m#torch.tanh(x_mod_pre)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mTout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_mod_pre_tanh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mlossG_target_mod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_27374/1231961149.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2_bn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3_bn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv4_bn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0;31m#print(x.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mleaky_relu\u001b[0;34m(input, negative_slope, inplace)\u001b[0m\n\u001b[1;32m   1473\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative_slope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1475\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative_slope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1476\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "writer2 = SummaryWriter(os.path.join('../celeimg', 'cls3_testzoo'))\n",
    "gpu1 = 1\n",
    "device1 = 1\n",
    "img_size = 64\n",
    "isCrop = False\n",
    "if isCrop:\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Scale(108),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "    ])\n",
    "else:\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "    ])\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "data_dir = '../gender'          # this path depends on your computer\n",
    "dset = datasets.ImageFolder(data_dir, transform)\n",
    "#train_size = int(0.8 * len(dset))\n",
    "#test_size = len(dset)- train_size\n",
    "#train_set, test_set = random_split(dset, [train_size, test_size])\n",
    "train_loader = torch.utils.data.DataLoader(dset, batch_size=bs,shuffle=True, drop_last=True)\n",
    "#test_loader = torch.utils.data.DataLoader(test_set, batch_size=100,shuffle=False)\n",
    "\n",
    "lr = 0.0002\n",
    "G2 = generator(128)\n",
    "D2 = discriminator(128)\n",
    "G2.weight_init(mean=0.0, std=0.02)\n",
    "D2.weight_init(mean=0.0, std=0.02)\n",
    "G2.cuda(gpu1)\n",
    "D2.cuda(gpu1)\n",
    "\n",
    "#criterion = nn.BCELoss().cuda(gpu)\n",
    "criterion = nn.CrossEntropyLoss().cuda(gpu1)\n",
    "# Adam optimizer\n",
    "G_optimizer2 = optim.Adam(G2.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "D_optimizer2 = optim.Adam(D2.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "D2.train()\n",
    "G2.train()\n",
    "for e in range(0, 800):\n",
    "    for real_x, real_y in train_loader:\n",
    "        real_x, real_y = real_x.to(gpu1), real_y.to(gpu1)\n",
    "        G2.zero_grad()\n",
    "        z = Variable(torch.randn(bs, 100)).view(-1, 100, 1, 1).to(gpu1)\n",
    "        fake_x = G2(z)\n",
    "        target_y = copy.deepcopy(real_y).fill_(0)\n",
    "        \n",
    "        grad_est = torch.zeros_like(fake_x).to(gpu1)\n",
    "        m = 20\n",
    "        epsilon = 0.1\n",
    "        N = fake_x.size(0)\n",
    "        C = fake_x.size(1)\n",
    "        S = fake_x.size(2)\n",
    "        d = S**2 * C\n",
    "        fake_img_tanh =fake_x#torch.tanh(fake_img)\n",
    "        lossG_target = criterion(D2(fake_img_tanh).squeeze(), target_y)\n",
    "        for i in range(m):\n",
    "            u = torch.randn(fake_x.size()).cuda(gpu1)\n",
    "            u_flat = u.view([N, -1])\n",
    "            u_norm = u / torch.norm(u_flat, dim=1).view([-1, 1, 1, 1])\n",
    "            x_mod_pre = fake_x + (epsilon * u_norm)\n",
    "            x_mod_pre_tanh = x_mod_pre #torch.tanh(x_mod_pre)\n",
    "            Tout = D2(x_mod_pre_tanh).squeeze()\n",
    "        \n",
    "            lossG_target_mod = criterion(Tout, target_y)\n",
    "            grad_est += (\n",
    "                (d / m) * (lossG_target_mod - lossG_target) / epsilon\n",
    "                ).view([-1, 1, 1, 1]) * u_norm\n",
    "        \n",
    "        grad_est /= N\n",
    "        g_loss = lossG_target_mod.mean()\n",
    "        fake_x.backward(grad_est)\n",
    "        G_optimizer2.step()\n",
    "    \n",
    "        D2.zero_grad()\n",
    "        fake_x2 = G2(z)\n",
    "        f_out = D2(fake_x2).squeeze()\n",
    "        d_fake_loss = criterion(f_out, copy.deepcopy(real_y).fill_(2))\n",
    "        r_out = D2(real_x).squeeze()\n",
    "        d_real_loss = criterion(r_out, real_y)\n",
    "        d_loss = d_fake_loss + d_real_loss\n",
    "        d_loss.backward()\n",
    "        D_optimizer2.step()\n",
    "    if e % 1 == 0:\n",
    "        print(f\"g_loss: {g_loss.item()}, d_loss: {d_loss.item()}\")\n",
    "    with torch.no_grad():\n",
    "        test_z = Variable(torch.randn(50, 100).view(-1, 100, 1, 1).to(device1))\n",
    "        generated = G2(test_z)\n",
    "        \n",
    "        out0grid = torchvision.utils.make_grid(generated, nrow=50)\n",
    "        writer2.add_image('images', out0grid, e)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51593fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
