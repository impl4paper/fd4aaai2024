{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "b927ba36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST, utils\n",
    "from PIL import Image\n",
    "import os.path\n",
    "import torch\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset\n",
    "from sampling import cifar_iid, cifar_noniid\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, Subset\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "4a5abe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_parties=10\n",
    "cls_use=[0,1, 7, 9, 12,18]\n",
    "num_samples=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6910677b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform_train = transforms.Compose([\n",
    "        transforms.Pad(4, padding_mode='reflect'),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop(32),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "        np.array([125.3, 123.0, 113.9]) / 255.0,\n",
    "        np.array([63.0, 62.1, 66.7]) / 255.0),])\n",
    "transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "        np.array([125.3, 123.0, 113.9]) / 255.0,\n",
    "        np.array([63.0, 62.1, 66.7]) / 255.0),])\n",
    "\n",
    "    \n",
    "trainset = datasets.CIFAR100(root='../data', train=True, download=True, transform=transform_train)\n",
    "testset = datasets.CIFAR100(root='../data', train=False, download=True, transform=transform_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "79d8cb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train= trainset.data,trainset.targets\n",
    "X_test, y_test = testset.data,testset.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "cf252ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "type(X_train),len(X_train),X_train[0].shape\n",
    "x,y = next(iter(DataLoader(trainset, batch_size=32)))\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "64b8d2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t = np.array(X_train)\n",
    "y_t = np.array(y_train)\n",
    "x_v = np.array(X_test)\n",
    "y_v = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a098892e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_t[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ffd08afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_partial_data(X, y, class_in_use = None, verbose = False):\n",
    "    if class_in_use is None:\n",
    "        idx = np.ones_like(y, dtype = bool)\n",
    "    else:\n",
    "        idx = [y == i for i in class_in_use]\n",
    "        idx = np.any(idx, axis = 0)\n",
    "    X_incomplete, y_incomplete = X[idx], y[idx]\n",
    "    if verbose == True:\n",
    "        print(\"X shape :\", X_incomplete.shape)\n",
    "        print(\"y shape :\", y_incomplete.shape)\n",
    "    return X_incomplete, y_incomplete\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "e23f8f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape : (3000, 32, 32, 3)\n",
      "y shape : (3000,)\n",
      "X shape : (600, 32, 32, 3)\n",
      "y shape : (600,)\n"
     ]
    }
   ],
   "source": [
    "x_t_p, y_t_p = generate_partial_data(X=x_t, y=y_t, class_in_use=cls_use, verbose=True)\n",
    "x_v_p , y_v_p = generate_partial_data(X=x_v, y=y_v, class_in_use=cls_use, verbose=True)\n",
    "private_cls_len = len(cls_use)\n",
    "public_cls_len = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "64efa8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, cls_ in enumerate(cls_use):\n",
    "    y_t_p[y_t_p == cls_] = index + public_cls_len\n",
    "    y_t_p[y_t_p == cls_] = index + public_cls_len\n",
    "del index, cls_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ef46a3de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 11, 13, 15, 15, 10, 14, 14, 13, 15, 11, 11, 14, 14, 10, 14, 10,\n",
       "       14, 10, 14])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_t_p[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "38d8c329",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_private_classes = np.arange(private_cls_len) + public_cls_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "abc525b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 11, 12, 13, 14, 15])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_private_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "144dd82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cifar_noniid(data_y, num_users):\n",
    "    \"\"\"\n",
    "    Sample non-I.I.D client data from CIFAR10 dataset\n",
    "    :param dataset:\n",
    "    :param num_users:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    num_shards, num_imgs = 20, 150\n",
    "    idx_shard = [i for i in range(num_shards)]\n",
    "    dict_users = {i: np.array([]) for i in range(num_users)}\n",
    "    idxs = np.arange(num_shards*num_imgs)\n",
    "    # labels = dataset.train_labels.numpy()\n",
    "    labels = np.array(data_y)\n",
    "\n",
    "    # sort labels\n",
    "    idxs_labels = np.vstack((idxs, labels))\n",
    "    idxs_labels = idxs_labels[:, idxs_labels[1, :].argsort()]\n",
    "    idxs = idxs_labels[0, :]\n",
    "\n",
    "    # divide and assign\n",
    "    for i in range(num_users):\n",
    "        rand_set = set(np.random.choice(idx_shard, 2, replace=False))\n",
    "        idx_shard = list(set(idx_shard) - rand_set)\n",
    "        for rand in rand_set:\n",
    "            dict_users[i] = np.concatenate(\n",
    "                (dict_users[i], idxs[rand*num_imgs:(rand+1)*num_imgs]), axis=0)\n",
    "    return dict_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ac6ab0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_index = cifar_noniid(y_t_p, N_parties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "256679df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_data(X_train_private, y_train_private,users_index,N_samples_per_class=10):\n",
    "    private_data,total_private_data = [],{'X':[],'y':[],'idx':[]}\n",
    "    for k in users_index.keys():\n",
    "        index = users_index[k]\n",
    "        idx = np.random.choice(range(len(index)),N_samples_per_class)\n",
    "        index_o = index[idx].astype(int)\n",
    "        private_data.append({'X':X_train_private[index_o],'y':y_train_private[index_o],'idx':index_o})\n",
    "        total_private_data['X'].extend(X_train_private[index_o].tolist())\n",
    "        total_private_data['y'].extend(y_train_private[index_o].tolist())\n",
    "        total_private_data['idx'].extend(index_o.tolist())\n",
    "\n",
    "    return private_data, total_private_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "0b1c3c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "private_data, total_private_data  = get_sample_data(x_t_p, y_t_p, users_index, num_samples*18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "68509551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, dict)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(private_data), type(total_private_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a3850c65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['X', 'y', 'idx'])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "private_data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a63b2497",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "93f7e741",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p_i in range(len(private_data)):\n",
    "    for i in range(len(private_data[p_i]['X'])):\n",
    "        trainset.append((private_data[p_i]['X'][i], private_data[p_i]['y'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "37e7cca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3600"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "15f46e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xdy/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "trainset = np.array(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "cf953ce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(users_index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "cbfc612b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape : (600, 32, 32, 3)\n",
      "y shape : (600,)\n"
     ]
    }
   ],
   "source": [
    "x_v_p , y_v_p = generate_partial_data(X=x_v, y=y_v, class_in_use=cls_use, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "ff38d5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xdy/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "testset = []\n",
    "for i in range(len(x_v_p)):\n",
    "    testset.append((x_v_p[i], y_v_p[i]))\n",
    "testset = np.array(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "11cdbe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetSplit_fix(Dataset):\n",
    "    \"\"\"An abstract Dataset class wrapped around Pytorch Dataset class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, idxs):\n",
    "        self.dataset = dataset\n",
    "        self.idxs = [int(i) for i in idxs]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image, label = self.dataset[self.idxs[item]]\n",
    "        return torch.tensor(image), torch.tensor(int(label))#  根据imbalanced-cifar10修改了这里\n",
    "testset = DatasetSplit_fix(testset, [i for i in range(len(testset))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "3e671534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 32, 32, 3])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y = next(iter(DataLoader(testset, batch_size=32)))\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a31197ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'noiddcifar100' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24381/3549145357.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnoiddcifar100\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'noiddcifar100' is not defined"
     ]
    }
   ],
   "source": [
    "t_set, v_set, num_dict = noiddcifar100()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1fee4810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def noiddcifar100(N_parties=10, cls_use=[0,1, 7, 9, 12,18], num_samples=20):\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.Pad(4, padding_mode='reflect'),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop(32),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "        np.array([125.3, 123.0, 113.9]) / 255.0,\n",
    "        np.array([63.0, 62.1, 66.7]) / 255.0),])\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "        np.array([125.3, 123.0, 113.9]) / 255.0,\n",
    "        np.array([63.0, 62.1, 66.7]) / 255.0),])\n",
    "\n",
    "    \n",
    "    trainset = datasets.CIFAR100(root='../data', train=True, download=True, transform=transform_train)\n",
    "    testset = datasets.CIFAR100(root='../data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "    X_train, y_train= trainset.data,trainset.targets\n",
    "    X_test, y_test = testset.data,testset.targets\n",
    "    x_t = np.array(X_train)\n",
    "    y_t = np.array(y_train)\n",
    "    x_v = np.array(X_test)\n",
    "    y_v = np.array(y_test)\n",
    "    x_t_p, y_t_p = generate_partial_data(X=x_t, y=y_t, class_in_use=cls_use, verbose=True)\n",
    "    x_v_p , y_v_p = generate_partial_data(X=x_v, y=y_v, class_in_use=cls_use, verbose=True)\n",
    "    private_cls_len = len(cls_use)\n",
    "    public_cls_len = 10\n",
    "    for index, cls_ in enumerate(cls_use):\n",
    "        y_t_p[y_t_p == cls_] = index + public_cls_len\n",
    "        y_t_p[y_t_p == cls_] = index + public_cls_len\n",
    "    del index, cls_\n",
    "\n",
    "    mod_private_classes = np.arange(private_cls_len) + public_cls_len\n",
    "    users_index = cifar_noniid(y_t_p, N_parties)\n",
    "\n",
    "    private_data, total_private_data  = get_sample_data(x_t_p, y_t_p, users_index, num_samples*18)\n",
    "    trainset = []\n",
    "    for p_i in range(len(private_data)):\n",
    "        for i in range(len(private_data[p_i]['X'])):\n",
    "            trainset.append((private_data[p_i]['X'][i], private_data[p_i]['y'][i]))\n",
    "    trainset = np.array(trainset)\n",
    "\n",
    "    testset = []\n",
    "    for i in range(len(x_v_p)):\n",
    "        testset.append((x_v_p[i], y_v_p[i]))\n",
    "    testset = np.array(testset)\n",
    "    testset = DatasetSplit_fix(testset, [i for i in range(len(testset))])\n",
    "    return trainset, testset, users_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "135bf3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "X shape : (3000, 32, 32, 3)\n",
      "y shape : (3000,)\n",
      "X shape : (600, 32, 32, 3)\n",
      "y shape : (600,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xdy/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:44: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "/home/xdy/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:49: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    }
   ],
   "source": [
    "t_set, v_set, num_dict = noiddcifar100()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bd99952f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(t_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "76e49b37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.DatasetSplit_fix"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(v_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1bba953c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = iter(t_set)\n",
    "x,y = next(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9643edca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[220, 187, 100],\n",
       "        [219, 190, 111],\n",
       "        [237, 207, 128],\n",
       "        ...,\n",
       "        [ 92,  96,  80],\n",
       "        [ 82,  87,  79],\n",
       "        [ 74,  75,  77]],\n",
       "\n",
       "       [[229, 201, 120],\n",
       "        [210, 192, 117],\n",
       "        [223, 206, 142],\n",
       "        ...,\n",
       "        [ 82,  86,  78],\n",
       "        [ 76,  83,  79],\n",
       "        [ 80,  83,  84]],\n",
       "\n",
       "       [[187, 160,  86],\n",
       "        [192, 169,  91],\n",
       "        [187, 161,  90],\n",
       "        ...,\n",
       "        [ 77,  80,  78],\n",
       "        [ 80,  87,  84],\n",
       "        [100, 104, 101]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[192, 162,  87],\n",
       "        [154, 123,  50],\n",
       "        [114,  93,  43],\n",
       "        ...,\n",
       "        [130,  62,  13],\n",
       "        [167,  90,  36],\n",
       "        [203, 114,  55]],\n",
       "\n",
       "       [[188, 159,  82],\n",
       "        [152, 121,  44],\n",
       "        [111,  90,  38],\n",
       "        ...,\n",
       "        [165,  90,  39],\n",
       "        [177,  96,  41],\n",
       "        [193, 105,  50]],\n",
       "\n",
       "       [[170, 143,  72],\n",
       "        [143, 116,  40],\n",
       "        [107,  90,  37],\n",
       "        ...,\n",
       "        [191, 102,  52],\n",
       "        [185,  98,  42],\n",
       "        [178,  91,  39]]], dtype=uint8)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ff76dd27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a19d685a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21d1565d11d4419bb90cdfa3551aca4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/cifar-10-python.tar.gz to ../data\n"
     ]
    }
   ],
   "source": [
    "trainset = datasets.CIFAR10(root='../data', train=True, download=True, transform=transform_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "79157391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__orig_bases__',\n",
       " '__parameters__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_check_integrity',\n",
       " '_format_transform_repr',\n",
       " '_load_meta',\n",
       " '_repr_indent',\n",
       " 'base_folder',\n",
       " 'class_to_idx',\n",
       " 'classes',\n",
       " 'data',\n",
       " 'download',\n",
       " 'extra_repr',\n",
       " 'filename',\n",
       " 'functions',\n",
       " 'meta',\n",
       " 'register_datapipe_as_function',\n",
       " 'register_function',\n",
       " 'root',\n",
       " 'target_transform',\n",
       " 'targets',\n",
       " 'test_list',\n",
       " 'tgz_md5',\n",
       " 'train',\n",
       " 'train_list',\n",
       " 'transform',\n",
       " 'transforms',\n",
       " 'url']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "efe99456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trainset.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e36a0cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "the size of publicset: 100\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Subset\n",
    "import random\n",
    "print(len(trainset))\n",
    "labels = torch.Tensor(trainset.targets)\n",
    "indices = []\n",
    "for i in range(10):\n",
    "    indices += random.sample((labels == i).nonzero().view(-1).tolist(), 10)\n",
    "publicset = Subset(trainset, indices)\n",
    "print(f\"the size of publicset: {len(publicset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5d2fb5ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.DatasetSplit_fix at 0x7f3001833250>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "090b5472",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "loader = DataLoader(testset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3f231ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = iter(loader)\n",
    "x, y = next(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e24368cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 32, 32, 3])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7990fc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tloader = DataLoader(DatasetSplit_fix(trainset, users_index[0]), batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "631ef584",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xdy/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "tloader = iter(tloader)\n",
    "x, y = next(tloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4ec9c79d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 3, 32, 32])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1c224384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 32, 32, 3)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_v_p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b485e394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 32, 32, 3)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_t_p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2c1bc983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_t_p[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4aacfd41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_v_p[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7a5e1c51",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'p' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24381/2989413824.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'p' is not defined"
     ]
    }
   ],
   "source": [
    "type(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "43690461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_temp = []\n",
    "t_temp.append((private_data[0]['X'][0], private_data[0]['y'][0]))\n",
    "t_temp[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "4391acc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "X shape : (3000, 3, 32, 32)\n",
      "y shape : (3000,)\n",
      "X shape : (600, 3, 32, 32)\n",
      "y shape : (600,)\n",
      "(3, 32, 32)\n",
      "(3, 32, 32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xdy/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:73: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "/home/xdy/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:78: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class DatasetSplit_fix(Dataset):\n",
    "    \"\"\"An abstract Dataset class wrapped around Pytorch Dataset class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, idxs):\n",
    "        self.dataset = dataset\n",
    "        self.idxs = [int(i) for i in idxs]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image, label = self.dataset[self.idxs[item]]\n",
    "        return torch.tensor(image), torch.tensor(int(label))#  根据imbalanced-cifar10修改了这里\n",
    "testset = DatasetSplit_fix(testset, [i for i in range(len(testset))])\n",
    "\n",
    "def cifar_noniid_fix(data_y, num_users):\n",
    "    \"\"\"\n",
    "    Sample non-I.I.D client data from CIFAR10 dataset\n",
    "    :param dataset:\n",
    "    :param num_users:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    num_shards, num_imgs = 20, 150\n",
    "    idx_shard = [i for i in range(num_shards)]\n",
    "    dict_users = {i: np.array([]) for i in range(num_users)}\n",
    "    idxs = np.arange(num_shards*num_imgs)\n",
    "    # labels = dataset.train_labels.numpy()\n",
    "    labels = np.array(data_y)\n",
    "\n",
    "    # sort labels\n",
    "    idxs_labels = np.vstack((idxs, labels))\n",
    "    idxs_labels = idxs_labels[:, idxs_labels[1, :].argsort()]\n",
    "    idxs = idxs_labels[0, :]\n",
    "\n",
    "    # divide and assign\n",
    "    for i in range(num_users):\n",
    "        rand_set = set(np.random.choice(idx_shard, 2, replace=False))\n",
    "        idx_shard = list(set(idx_shard) - rand_set)\n",
    "        for rand in rand_set:\n",
    "            dict_users[i] = np.concatenate(\n",
    "                (dict_users[i], idxs[rand*num_imgs:(rand+1)*num_imgs]), axis=0)\n",
    "    return dict_users\n",
    "\n",
    "\n",
    "def noiddcifar100(N_parties=10, cls_use=[0,1, 7, 9, 12,18], num_samples=20):\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.Pad(4, padding_mode='reflect'),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop(32),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "        np.array([125.3, 123.0, 113.9]) / 255.0,\n",
    "        np.array([63.0, 62.1, 66.7]) / 255.0),])\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "        np.array([125.3, 123.0, 113.9]) / 255.0,\n",
    "        np.array([63.0, 62.1, 66.7]) / 255.0),])\n",
    "\n",
    "    \n",
    "    trainset = datasets.CIFAR100(root='../data', train=True, download=True, transform=transform_train)\n",
    "    testset = datasets.CIFAR100(root='../data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "    X_train, y_train= trainset.data,trainset.targets\n",
    "    X_test, y_test = testset.data,testset.targets\n",
    "    x_t = np.transpose(X_train, (0,3, 1,2))\n",
    "    y_t = np.array(y_train)\n",
    "    x_v = np.transpose(X_test, (0,3,1,2))\n",
    "    y_v = np.array(y_test)\n",
    "    x_t_p, y_t_p = generate_partial_data(X=x_t, y=y_t, class_in_use=cls_use, verbose=True)\n",
    "    x_v_p , y_v_p = generate_partial_data(X=x_v, y=y_v, class_in_use=cls_use, verbose=True)\n",
    "    private_cls_len = len(cls_use)\n",
    "    public_cls_len = 10\n",
    "    for index, cls_ in enumerate(cls_use):\n",
    "        y_t_p[y_t_p == cls_] = index + public_cls_len\n",
    "        y_t_p[y_t_p == cls_] = index + public_cls_len\n",
    "    del index, cls_\n",
    "\n",
    "    mod_private_classes = np.arange(private_cls_len) + public_cls_len\n",
    "    users_index = cifar_noniid_fix(y_t_p, N_parties)\n",
    "\n",
    "    private_data, total_private_data  = get_sample_data(x_t_p, y_t_p, users_index, num_samples*18)\n",
    "    trainset = []\n",
    "    for p_i in range(len(private_data)):\n",
    "        for i in range(len(private_data[p_i]['X'])):\n",
    "            trainset.append((private_data[p_i]['X'][i], private_data[p_i]['y'][i]))\n",
    "    trainset = np.array(trainset)\n",
    "    print(trainset[0][0].shape)\n",
    "    testset = []\n",
    "    for i in range(len(x_v_p)):\n",
    "        testset.append((x_v_p[i], y_v_p[i]))\n",
    "    testset = np.array(testset)\n",
    "    print(testset[0][0].shape)\n",
    "    testset = DatasetSplit_fix(testset, [i for i in range(len(testset))])\n",
    "    return trainset, testset, users_index\n",
    "trainset,testset,user_index  = noiddcifar100()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "0aecf800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 32, 32), torch.Size([3, 32, 32]))"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[0][0].shape, testset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "58b5d4ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trainset[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "4765b147",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(DataLoader(DatasetSplit_fix(trainset, users_index[0]), batch_size=128)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "105b7dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[143, 141, 141,  ..., 150, 145, 114],\n",
       "          [150, 149, 145,  ..., 187, 183, 145],\n",
       "          [131, 134, 137,  ..., 169, 165, 140],\n",
       "          ...,\n",
       "          [229, 236, 237,  ..., 235, 234, 199],\n",
       "          [228, 235, 238,  ..., 233, 231, 197],\n",
       "          [225, 233, 235,  ..., 232, 232, 197]],\n",
       "\n",
       "         [[137, 137, 136,  ..., 126, 121,  97],\n",
       "          [145, 143, 140,  ..., 177, 174, 139],\n",
       "          [131, 134, 137,  ..., 165, 163, 140],\n",
       "          ...,\n",
       "          [206, 213, 214,  ..., 212, 209, 176],\n",
       "          [207, 214, 216,  ..., 209, 206, 174],\n",
       "          [205, 212, 214,  ..., 209, 207, 175]],\n",
       "\n",
       "         [[ 96,  94,  91,  ...,  73,  70,  56],\n",
       "          [ 99,  97,  93,  ..., 134, 136, 110],\n",
       "          [ 88,  89,  92,  ..., 127, 128, 112],\n",
       "          ...,\n",
       "          [160, 165, 167,  ..., 163, 160, 134],\n",
       "          [161, 167, 170,  ..., 160, 157, 133],\n",
       "          [160, 167, 169,  ..., 160, 158, 134]]],\n",
       "\n",
       "\n",
       "        [[[255, 255, 255,  ..., 255, 255, 255],\n",
       "          [255, 254, 254,  ..., 254, 254, 255],\n",
       "          [255, 254, 255,  ..., 255, 254, 255],\n",
       "          ...,\n",
       "          [255, 254, 255,  ..., 254, 254, 255],\n",
       "          [255, 254, 254,  ..., 254, 254, 255],\n",
       "          [255, 255, 255,  ..., 255, 255, 255]],\n",
       "\n",
       "         [[255, 255, 255,  ..., 255, 255, 255],\n",
       "          [255, 254, 254,  ..., 254, 254, 255],\n",
       "          [255, 254, 255,  ..., 255, 254, 255],\n",
       "          ...,\n",
       "          [255, 254, 255,  ..., 254, 254, 255],\n",
       "          [255, 254, 254,  ..., 254, 254, 255],\n",
       "          [255, 255, 255,  ..., 255, 255, 255]],\n",
       "\n",
       "         [[255, 255, 255,  ..., 255, 255, 255],\n",
       "          [255, 254, 254,  ..., 254, 254, 255],\n",
       "          [255, 254, 255,  ..., 255, 254, 255],\n",
       "          ...,\n",
       "          [255, 254, 255,  ..., 253, 254, 255],\n",
       "          [255, 254, 254,  ..., 253, 254, 255],\n",
       "          [255, 255, 255,  ..., 255, 255, 255]]],\n",
       "\n",
       "\n",
       "        [[[136, 160, 142,  ..., 147, 133, 112],\n",
       "          [127, 160, 154,  ..., 177, 153, 132],\n",
       "          [125, 134, 145,  ..., 166, 141, 138],\n",
       "          ...,\n",
       "          [ 91,  98, 115,  ..., 124, 114, 116],\n",
       "          [104, 102, 119,  ..., 108, 114, 113],\n",
       "          [112, 116, 118,  ..., 109, 112, 111]],\n",
       "\n",
       "         [[143, 167, 149,  ..., 158, 144, 123],\n",
       "          [134, 167, 161,  ..., 188, 164, 143],\n",
       "          [132, 141, 152,  ..., 177, 152, 149],\n",
       "          ...,\n",
       "          [102, 109, 126,  ..., 135, 125, 127],\n",
       "          [115, 113, 130,  ..., 119, 125, 123],\n",
       "          [123, 127, 129,  ..., 120, 123, 122]],\n",
       "\n",
       "         [[136, 160, 142,  ..., 154, 140, 119],\n",
       "          [127, 160, 154,  ..., 184, 160, 139],\n",
       "          [125, 134, 145,  ..., 173, 148, 145],\n",
       "          ...,\n",
       "          [ 95, 105, 124,  ..., 127, 119, 123],\n",
       "          [109, 109, 128,  ..., 111, 120, 120],\n",
       "          [116, 123, 127,  ..., 112, 117, 118]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[129, 130, 129,  ..., 116, 114, 113],\n",
       "          [126, 127, 126,  ..., 116, 114, 113],\n",
       "          [126, 125, 126,  ..., 114, 114, 109],\n",
       "          ...,\n",
       "          [101,  97,  91,  ...,  74,  87,  92],\n",
       "          [ 96,  95,  93,  ...,  83,  84,  81],\n",
       "          [101,  95,  97,  ...,  90,  94,  88]],\n",
       "\n",
       "         [[139, 135, 136,  ..., 123, 122, 123],\n",
       "          [139, 136, 136,  ..., 123, 122, 123],\n",
       "          [137, 134, 133,  ..., 124, 121, 124],\n",
       "          ...,\n",
       "          [239, 238, 236,  ..., 228, 236, 239],\n",
       "          [242, 241, 238,  ..., 230, 227, 232],\n",
       "          [243, 237, 237,  ..., 239, 234, 235]],\n",
       "\n",
       "         [[154, 151, 154,  ..., 149, 148, 150],\n",
       "          [156, 152, 154,  ..., 149, 147, 149],\n",
       "          [155, 153, 154,  ..., 150, 149, 151],\n",
       "          ...,\n",
       "          [255, 252, 252,  ..., 253, 253, 253],\n",
       "          [255, 252, 251,  ..., 253, 253, 254],\n",
       "          [255, 250, 251,  ..., 254, 254, 254]]],\n",
       "\n",
       "\n",
       "        [[[ 64,   5,  23,  ...,  10,   7,  11],\n",
       "          [103,   3,  21,  ...,   9,  15,  30],\n",
       "          [ 78,   4,  20,  ...,   9,  12,  19],\n",
       "          ...,\n",
       "          [  8,  19,  21,  ...,  41,  50,  17],\n",
       "          [ 12,  12,  17,  ...,  38,  35,  29],\n",
       "          [  8,   5,   9,  ...,  24,  21,  35]],\n",
       "\n",
       "         [[ 69,  10,  19,  ...,  15,  17,  26],\n",
       "          [110,  10,  15,  ...,  15,  24,  43],\n",
       "          [ 82,  11,  13,  ...,  16,  20,  29],\n",
       "          ...,\n",
       "          [ 36,  48,  47,  ...,  86, 102,  70],\n",
       "          [ 30,  28,  31,  ...,  84,  87,  85],\n",
       "          [ 20,  14,  17,  ...,  64,  68,  92]],\n",
       "\n",
       "         [[ 57,  29,  55,  ...,  35,  47,  79],\n",
       "          [104,  33,  55,  ...,  40,  59, 100],\n",
       "          [ 87,  40,  57,  ...,  43,  56,  85],\n",
       "          ...,\n",
       "          [ 91, 100,  97,  ..., 127, 147, 126],\n",
       "          [ 88,  83,  83,  ..., 124, 130, 135],\n",
       "          [ 79,  70,  70,  ..., 104, 110, 137]]],\n",
       "\n",
       "\n",
       "        [[[ 68,  71,  67,  ...,  43,  43,  46],\n",
       "          [ 52,  50,  54,  ...,  44,  47,  60],\n",
       "          [ 31,  33,  43,  ...,  44,  73,  73],\n",
       "          ...,\n",
       "          [ 29,  31,  23,  ...,  23,  36,  40],\n",
       "          [ 29,  33,  30,  ...,  22,  30,  33],\n",
       "          [ 26,  28,  40,  ...,  26,  28,  31]],\n",
       "\n",
       "         [[ 31,  28,  26,  ...,  39,  36,  41],\n",
       "          [ 33,  25,  27,  ...,  38,  42,  57],\n",
       "          [ 31,  19,  18,  ...,  39,  70,  71],\n",
       "          ...,\n",
       "          [ 25,  24,  24,  ...,  23,  28,  30],\n",
       "          [ 24,  26,  30,  ...,  21,  24,  27],\n",
       "          [ 21,  24,  36,  ...,  21,  23,  26]],\n",
       "\n",
       "         [[ 56,  47,  39,  ...,  59,  64,  59],\n",
       "          [ 57,  49,  51,  ...,  67,  60,  65],\n",
       "          [ 52,  48,  52,  ...,  64,  78,  73],\n",
       "          ...,\n",
       "          [ 60,  60,  58,  ...,  49,  55,  57],\n",
       "          [ 59,  62,  66,  ...,  49,  55,  57],\n",
       "          [ 55,  57,  70,  ...,  51,  53,  56]]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "e063bd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "display-im6.q16: unable to open X server `' @ error/display.c/DisplayImageCommand/432.\n"
     ]
    }
   ],
   "source": [
    "func = transforms.ToPILImage()\n",
    "img = func(x[0])\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2fdc5c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xdy/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "x,y = next(iter(DataLoader(DatasetSplit_fix(testset, [i for i in range(200)]), batch_size=128)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "287f6059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 32, 32, 3])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "2e261bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_EMNIST_data(file=r'../data/emnist-letters.mat', verbose = True, standarized = False):\n",
    "    \"\"\"\n",
    "    file should be the downloaded EMNIST file in .mat format.\n",
    "    \"\"\"    \n",
    "    mat = sio.loadmat(file)\n",
    "    data = mat[\"dataset\"]\n",
    "    \n",
    "\n",
    "    writer_ids_train = data['train'][0,0]['writers'][0,0]\n",
    "    writer_ids_train = np.squeeze(writer_ids_train)\n",
    "    X_train = data['train'][0,0]['images'][0,0]\n",
    "    X_train = X_train.reshape((X_train.shape[0], 28, 28), order = \"F\")\n",
    "    X_train = X_train[:, np.newaxis, :, :]\n",
    "    y_train = data['train'][0,0]['labels'][0,0]\n",
    "    y_train = np.squeeze(y_train)\n",
    "    y_train -= 1 #y_train is zero-based\n",
    "    \n",
    "    writer_ids_test = data['test'][0,0]['writers'][0,0]\n",
    "    writer_ids_test = np.squeeze(writer_ids_test)\n",
    "    X_test = data['test'][0,0]['images'][0,0]\n",
    "    X_test= X_test.reshape((X_test.shape[0], 28, 28), order = \"F\")\n",
    "    X_test = X_test[:, np.newaxis, :, :]\n",
    "    y_test = data['test'][0,0]['labels'][0,0]\n",
    "    y_test = np.squeeze(y_test)\n",
    "    y_test -= 1 #y_test is zero-based\n",
    "\n",
    "    \n",
    "    if standarized: \n",
    "        X_train = X_train/255\n",
    "        X_test = X_test/255\n",
    "        mean_image = np.mean(X_train, axis=0)\n",
    "        X_train -= mean_image\n",
    "        X_test -= mean_image\n",
    "    \n",
    "\n",
    "    if verbose == True: \n",
    "        print(\"EMNIST-letter dataset ... \")\n",
    "        print(\"X_train shape :\", X_train.shape)\n",
    "        print(\"X_test shape :\", X_test.shape)\n",
    "        print(\"y_train shape :\", y_train.shape)\n",
    "        print(\"y_test shape :\", y_test.shape)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, writer_ids_train, writer_ids_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "6c7b38ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMNIST-letter dataset ... \n",
      "X_train shape : (124800, 1, 28, 28)\n",
      "X_test shape : (20800, 1, 28, 28)\n",
      "y_train shape : (124800,)\n",
      "y_test shape : (20800,)\n"
     ]
    }
   ],
   "source": [
    "a,b,c,d,e,f = load_EMNIST_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "ccbf7081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform_train = transforms.Compose([\n",
    "        transforms.Pad(4, padding_mode='reflect'),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop(32),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "        np.array([125.3, 123.0, 113.9]) / 255.0,\n",
    "        np.array([63.0, 62.1, 66.7]) / 255.0),])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "        np.array([125.3, 123.0, 113.9]) / 255.0,\n",
    "        np.array([63.0, 62.1, 66.7]) / 255.0),])\n",
    "\n",
    "trainset = datasets.CIFAR100(root='../data', train=True, download=True, transform=transform_train)\n",
    "testset = datasets.CIFAR100(root='../data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "train_labels,test_labels = torch.Tensor(trainset.targets), torch.Tensor(testset.targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "3eb8db37",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_use=[0,1, 7, 9, 12,18]\n",
    "t_indices, v_indices = [],[]\n",
    "for i in cls_use:\n",
    "    t_indices += random.sample((train_labels == i).nonzero().view(-1).tolist(), 500)\n",
    "    v_indices += random.sample((test_labels == i).nonzero().view(-1).tolist(), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "b9ebda06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 600)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t_indices), len(v_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "e01735d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset = Subset(trainset, t_indices)\n",
    "len(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "435fdf92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "930197ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Subset' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24381/3760096731.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls_use\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0msubset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m  \u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpublic_cls_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'Subset' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "private_cls_len = len(cls_use)\n",
    "public_cls_len = 10\n",
    "for i in range(len(subset)):\n",
    "    x, y = subset[i]\n",
    "    for index, cls in enumerate(cls_use):\n",
    "        if y == cls:\n",
    "            subset[i]=  (subset[i][0], (index + public_cls_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "88fa5619",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetSplit_fix(Dataset):\n",
    "    \"\"\"An abstract Dataset class wrapped around Pytorch Dataset class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, idxs):\n",
    "        cls = [0, 1, 7, 9, 12, 18]\n",
    "        self.cls_use = {cls[i]: i + 10 for i in range(len(cls))}\n",
    "        #self.cls_use = {0:10, 1: 11, 7: 12, 9: 13, 12: 14, 18: 15}\n",
    "        self.cls_use_len = len(self.cls_use)\n",
    "        self.dataset = dataset\n",
    "        self.idxs = [int(i) for i in idxs]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image, label = self.dataset[self.idxs[item]]\n",
    "        return torch.tensor(image), torch.tensor(self.cls_use[int(label)])#  根据imbalanced-cifar10修改了这里\n",
    "    \n",
    "    \n",
    "    \n",
    "traindata = DatasetSplit_fix(subset, [i for i in range(3000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "81868c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n",
      "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n",
      "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n",
      "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n",
      "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n",
      "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n",
      "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n",
      "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n",
      "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n",
      "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n",
      "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n",
      "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n",
      "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n",
      "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n",
      "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n",
      "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n",
      "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n",
      "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n",
      "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n",
      "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n",
      "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n",
      "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n",
      "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n",
      "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n",
      "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n",
      "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xdy/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n",
      "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n",
      "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n",
      "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n",
      "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n",
      "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n",
      "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n",
      "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n",
      "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n",
      "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n",
      "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n",
      "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n",
      "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n",
      "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n",
      "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n",
      "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n",
      "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n",
      "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n",
      "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n",
      "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n",
      "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n",
      "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n",
      "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n",
      "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n",
      "tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11])\n",
      "tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11])\n",
      "tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11])\n",
      "tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11])\n",
      "tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11])\n",
      "tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11])\n",
      "tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11])\n",
      "tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11])\n",
      "tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11])\n",
      "tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11])\n",
      "tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11])\n",
      "tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11])\n",
      "tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11])\n",
      "tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11])\n",
      "tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11])\n",
      "tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11])\n",
      "tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11])\n",
      "tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11])\n",
      "tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11])\n",
      "tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11])\n",
      "tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11])\n",
      "tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11])\n",
      "tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11])\n",
      "tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11])\n",
      "tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11])\n",
      "tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11])\n",
      "tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11])\n",
      "tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11])\n",
      "tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11])\n",
      "tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11])\n",
      "tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11])\n",
      "tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11])\n",
      "tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11])\n",
      "tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11])\n",
      "tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11])\n",
      "tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11])\n",
      "tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11])\n",
      "tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11])\n",
      "tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11])\n",
      "tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11])\n",
      "tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11])\n",
      "tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11])\n",
      "tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11])\n",
      "tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11])\n",
      "tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11])\n",
      "tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11])\n",
      "tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11])\n",
      "tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11])\n",
      "tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11])\n",
      "tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13])\n",
      "tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13])\n",
      "tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13])\n",
      "tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13])\n",
      "tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13])\n",
      "tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13])\n",
      "tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13])\n",
      "tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13])\n",
      "tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13])\n",
      "tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13])\n",
      "tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13])\n",
      "tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13])\n",
      "tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13])\n",
      "tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13])\n",
      "tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13])\n",
      "tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13])\n",
      "tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13])\n",
      "tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13])\n",
      "tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13])\n",
      "tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13])\n",
      "tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13])\n",
      "tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13])\n",
      "tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13])\n",
      "tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13])\n",
      "tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13])\n",
      "tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13])\n",
      "tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13])\n",
      "tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13])\n",
      "tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13])\n",
      "tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13])\n",
      "tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13])\n",
      "tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13])\n",
      "tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13])\n",
      "tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13])\n",
      "tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13])\n",
      "tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13])\n",
      "tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13])\n",
      "tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13])\n",
      "tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13])\n",
      "tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13])\n",
      "tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13])\n",
      "tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13])\n",
      "tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13])\n",
      "tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13])\n",
      "tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13])\n",
      "tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13])\n",
      "tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13])\n",
      "tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13])\n",
      "tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13])\n",
      "tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([14, 14, 14, 14, 14, 14, 14, 14, 14, 14])\n",
      "tensor([14, 14, 14, 14, 14, 14, 14, 14, 14, 14])\n",
      "tensor([14, 14, 14, 14, 14, 14, 14, 14, 14, 14])\n",
      "tensor([14, 14, 14, 14, 14, 14, 14, 14, 14, 14])\n",
      "tensor([14, 14, 14, 14, 14, 14, 14, 14, 14, 14])\n",
      "tensor([14, 14, 14, 14, 14, 14, 14, 14, 14, 14])\n",
      "tensor([14, 14, 14, 14, 14, 14, 14, 14, 14, 14])\n",
      "tensor([14, 14, 14, 14, 14, 14, 14, 14, 14, 14])\n",
      "tensor([14, 14, 14, 14, 14, 14, 14, 14, 14, 14])\n",
      "tensor([14, 14, 14, 14, 14, 14, 14, 14, 14, 14])\n",
      "tensor([14, 14, 14, 14, 14, 14, 14, 14, 14, 14])\n",
      "tensor([14, 14, 14, 14, 14, 14, 14, 14, 14, 14])\n",
      "tensor([14, 14, 14, 14, 14, 14, 14, 14, 14, 14])\n",
      "tensor([14, 14, 14, 14, 14, 14, 14, 14, 14, 14])\n",
      "tensor([14, 14, 14, 14, 14, 14, 14, 14, 14, 14])\n",
      "tensor([14, 14, 14, 14, 14, 14, 14, 14, 14, 14])\n",
      "tensor([14, 14, 14, 14, 14, 14, 14, 14, 14, 14])\n",
      "tensor([14, 14, 14, 14, 14, 14, 14, 14, 14, 14])\n",
      "tensor([14, 14, 14, 14, 14, 14, 14, 14, 14, 14])\n",
      "tensor([14, 14, 14, 14, 14, 14, 14, 14, 14, 14])\n",
      "tensor([14, 14, 14, 14, 14, 14, 14, 14, 14, 14])\n",
      "tensor([14, 14, 14, 14, 14, 14, 14, 14, 14, 14])\n",
      "tensor([14, 14, 14, 14, 14, 14, 14, 14, 14, 14])\n",
      "tensor([14, 14, 14, 14, 14, 14, 14, 14, 14, 14])\n",
      "tensor([14, 14, 14, 14, 14, 14, 14, 14, 14, 14])\n",
      "tensor([14, 14, 14, 14, 14, 14, 14, 14, 14, 14])\n",
      "tensor([14, 14, 14, 14, 14, 14, 14, 14, 14, 14])\n",
      "tensor([14, 14, 14, 14, 14, 14, 14, 14, 14, 14])\n",
      "tensor([14, 14, 14, 14, 14, 14, 14, 14, 14, 14])\n",
      "tensor([14, 14, 14, 14, 14, 14, 14, 14, 14, 14])\n",
      "tensor([14, 14, 14, 14, 14, 14, 14, 14, 14, 14])\n",
      "tensor([14, 14, 14, 14, 14, 14, 14, 14, 14, 14])\n",
      "tensor([14, 14, 14, 14, 14, 14, 14, 14, 14, 14])\n",
      "tensor([14, 14, 14, 14, 14, 14, 14, 14, 14, 14])\n",
      "tensor([14, 14, 14, 14, 14, 14, 14, 14, 14, 14])\n",
      "tensor([14, 14, 14, 14, 14, 14, 14, 14, 14, 14])\n",
      "tensor([14, 14, 14, 14, 14, 14, 14, 14, 14, 14])\n",
      "tensor([14, 14, 14, 14, 14, 14, 14, 14, 14, 14])\n",
      "tensor([14, 14, 14, 14, 14, 14, 14, 14, 14, 14])\n",
      "tensor([14, 14, 14, 14, 14, 14, 14, 14, 14, 14])\n",
      "tensor([14, 14, 14, 14, 14, 14, 14, 14, 14, 14])\n",
      "tensor([14, 14, 14, 14, 14, 14, 14, 14, 14, 14])\n",
      "tensor([14, 14, 14, 14, 14, 14, 14, 14, 14, 14])\n",
      "tensor([14, 14, 14, 14, 14, 14, 14, 14, 14, 14])\n",
      "tensor([14, 14, 14, 14, 14, 14, 14, 14, 14, 14])\n",
      "tensor([14, 14, 14, 14, 14, 14, 14, 14, 14, 14])\n",
      "tensor([14, 14, 14, 14, 14, 14, 14, 14, 14, 14])\n",
      "tensor([14, 14, 14, 14, 14, 14, 14, 14, 14, 14])\n",
      "tensor([14, 14, 14, 14, 14, 14, 14, 14, 14, 14])\n",
      "tensor([14, 14, 14, 14, 14, 14, 14, 14, 14, 14])\n",
      "tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15])\n",
      "tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15])\n",
      "tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15])\n",
      "tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15])\n",
      "tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15])\n",
      "tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15])\n",
      "tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15])\n",
      "tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15])\n",
      "tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15])\n",
      "tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15])\n",
      "tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15])\n",
      "tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15])\n",
      "tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15])\n",
      "tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15])\n",
      "tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15])\n",
      "tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15])\n",
      "tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15])\n",
      "tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15])\n",
      "tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15])\n",
      "tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15])\n",
      "tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15])\n",
      "tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15])\n",
      "tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15])\n",
      "tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15])\n",
      "tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15])\n",
      "tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15])\n",
      "tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15])\n",
      "tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15])\n",
      "tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15])\n",
      "tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15])\n",
      "tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15])\n",
      "tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15])\n",
      "tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15])\n",
      "tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15])\n",
      "tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15])\n",
      "tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15])\n",
      "tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15])\n",
      "tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15])\n",
      "tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15])\n",
      "tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15])\n",
      "tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15])\n",
      "tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15])\n",
      "tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15])\n",
      "tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15])\n",
      "tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15])\n",
      "tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15])\n",
      "tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15])\n",
      "tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15])\n",
      "tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15])\n",
      "tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15])\n"
     ]
    }
   ],
   "source": [
    "print(len(traindata))\n",
    "loader = DataLoader(traindata, batch_size=10)\n",
    "for index, (x, y) in enumerate(loader):\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "a904c6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cifar_noniid_fix(data_y, num_users):\n",
    "    \"\"\"\n",
    "    Sample non-I.I.D client data from CIFAR10 dataset\n",
    "    :param dataset:\n",
    "    :param num_users:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    num_shards, num_imgs = 20, 150\n",
    "    idx_shard = [i for i in range(num_shards)]\n",
    "    dict_users = {i: np.array([]) for i in range(num_users)}\n",
    "    idxs = np.arange(num_shards*num_imgs)\n",
    "    # labels = dataset.train_labels.numpy()\n",
    "    labels = np.array(data_y)\n",
    "\n",
    "    # sort labels\n",
    "    idxs_labels = np.vstack((idxs, labels))\n",
    "    idxs_labels = idxs_labels[:, idxs_labels[1, :].argsort()]\n",
    "    idxs = idxs_labels[0, :]\n",
    "\n",
    "    # divide and assign\n",
    "    for i in range(num_users):\n",
    "        rand_set = set(np.random.choice(idx_shard, 2, replace=False))\n",
    "        idx_shard = list(set(idx_shard) - rand_set)\n",
    "        for rand in rand_set:\n",
    "            dict_users[i] = np.concatenate(\n",
    "                (dict_users[i], idxs[rand*num_imgs:(rand+1)*num_imgs]), axis=0)\n",
    "    return dict_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "efb2b80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_noniid(dataset, num_users, train_flag=True):\n",
    "    \"\"\"\n",
    "    Sample non-I.I.D client data from MNIST dataset\n",
    "    :param dataset:\n",
    "    :param num_users:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if train_flag:\n",
    "        num_shards, num_imgs = 200, 300\n",
    "    else:\n",
    "        num_shards, num_imgs = 25, 400\n",
    "    idx_shard = [i for i in range(num_shards)]\n",
    "    dict_users = {i: np.array([], dtype='int64') for i in range(num_users)}\n",
    "    idxs = np.arange(num_shards*num_imgs)\n",
    "    labels = dataset.targets.numpy()\n",
    "    # sort labels\n",
    "    idxs_labels = np.vstack((idxs, labels))\n",
    "    idxs_labels = idxs_labels[:,idxs_labels[1,:].argsort()]\n",
    "    idxs = idxs_labels[0,:]\n",
    "\n",
    "    # divide and assign\n",
    "    for i in range(num_users):\n",
    "        rand_set = set(np.random.choice(idx_shard, 2, replace=False))\n",
    "        idx_shard = list(set(idx_shard) - rand_set)\n",
    "        for rand in rand_set:\n",
    "            dict_users[i] = np.concatenate((dict_users[i], idxs[rand*num_imgs:(rand+1)*num_imgs]), axis=0)\n",
    "    return dict_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "dfe65fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ourcifar_noniid(dataset, no_participants, alpha=0.9):\n",
    "    \"\"\"\n",
    "    Input: Number of participants and alpha (param for distribution)\n",
    "    Output: A list of indices denoting data in CIFAR training set.\n",
    "    Requires: cifar_classes, a preprocessed class-indice dictionary.\n",
    "    Sample Method: take a uniformly sampled 10-dimension vector as parameters for\n",
    "    dirichlet distribution to sample number of images in each class.\n",
    "    \"\"\"\n",
    "    cifar_classes = {}\n",
    "    for ind, x in enumerate(dataset):\n",
    "        _, label = x\n",
    "        if label in cifar_classes:\n",
    "            cifar_classes[label].append(ind)\n",
    "        else:\n",
    "            cifar_classes[label] = [ind]\n",
    "\n",
    "    per_participant_list = defaultdict(list)\n",
    "    no_classes = len(cifar_classes.keys())\n",
    "    class_size = len(cifar_classes[0])\n",
    "    datasize = {}\n",
    "    print(cifar_classes.keys())\n",
    "    for n in range(no_classes):\n",
    "        random.shuffle(cifar_classes[n])\n",
    "        sampled_probabilities = class_size * np.random.dirichlet(\n",
    "            np.array(no_participants * [alpha]))\n",
    "        for user in range(no_participants):\n",
    "            no_imgs = int(round(sampled_probabilities[user]))\n",
    "            datasize[user, n] = no_imgs\n",
    "            sampled_list = cifar_classes[n][:min(len(cifar_classes[n]), no_imgs)]\n",
    "            per_participant_list[user].extend(sampled_list)\n",
    "            cifar_classes[n] = cifar_classes[n][min(len(cifar_classes[n]), no_imgs):]\n",
    "    train_img_size = np.zeros(no_participants)\n",
    "    for i in range(no_participants):\n",
    "        train_img_size[i] = sum([datasize[i,j] for j in range(10)])\n",
    "    clas_weight = np.zeros((no_participants,10))\n",
    "    for i in range(no_participants):\n",
    "        for j in range(10):\n",
    "            clas_weight[i,j] = float(datasize[i,j])/float((train_img_size[i]))\n",
    "    for i in per_participant_list:\n",
    "        np.random.shuffle(per_participant_list[i])\n",
    "    return per_participant_list, clas_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "c8c7271b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([0, 1, 7, 9, 12, 18])\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24381/475686092.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdict_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mourcifar_noniid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_24381/1530276812.py\u001b[0m in \u001b[0;36mourcifar_noniid\u001b[0;34m(dataset, no_participants, alpha)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcifar_classes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcifar_classes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         sampled_probabilities = class_size * np.random.dirichlet(\n\u001b[1;32m     25\u001b[0m             np.array(no_participants * [alpha]))\n",
      "\u001b[0;31mKeyError\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "dict_num, _ =ourcifar_noniid(subset, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "9f397537",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24381/1106874311.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msubset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mtuple\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mwhere\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mindex\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32mclass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \"\"\"\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;31m# doing this so that it is consistent with all other datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not list"
     ]
    }
   ],
   "source": [
    "subset[[i for i in range(10)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "cf33ad50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSubset(Subset):\n",
    "    '''A custom subset class'''\n",
    "    def __init__(self, dataset, indices):\n",
    "        super().__init__(dataset, indices)\n",
    "        self.targets = dataset.targets # 保留targets属性\n",
    "        self.classes = dataset.classes # 保留classes属性\n",
    "\n",
    "    def __getitem__(self, idx): #同时支持索引访问操作\n",
    "        x, y = self.dataset[self.indices[idx]]      \n",
    "        return x, y \n",
    "\n",
    "    def __len__(self): # 同时支持取长度操作\n",
    "        return len(self.indices)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_cifar100_with_cls(cls_use=[0,1, 7, 9, 12,18]):\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.Pad(4, padding_mode='reflect'),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop(32),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "        np.array([125.3, 123.0, 113.9]) / 255.0,\n",
    "        np.array([63.0, 62.1, 66.7]) / 255.0),])\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "        np.array([125.3, 123.0, 113.9]) / 255.0,\n",
    "        np.array([63.0, 62.1, 66.7]) / 255.0),])\n",
    "\n",
    "    \n",
    "    trainset = datasets.CIFAR100(root='../data', train=True, download=True, transform=transform_train)\n",
    "    testset = datasets.CIFAR100(root='../data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "    train_labels,test_labels = torch.Tensor(trainset.targets), torch.Tensor(testset.targets)\n",
    "\n",
    "    t_indices, v_indices = [],[]\n",
    "    for i in cls_use:\n",
    "        t_indices += random.sample((train_labels == i).nonzero().view(-1).tolist(), 500)\n",
    "        v_indices += random.sample((test_labels == i).nonzero().view(-1).tolist(), 100)\n",
    "\n",
    "    train_subset = CustomSubset(trainset, t_indices)\n",
    "    test_subset = CustomSubset(testset, v_indices)\n",
    "    print(f\"the size of training subset of cifar100: {len(train_subset)}\")\n",
    "    print(f\"the size of testing subset of cifar100: {len(test_subset)}\")\n",
    "    return train_subset, test_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "66cd32e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "the size of training subset of cifar100: 3000\n",
      "the size of testing subset of cifar100: 600\n"
     ]
    }
   ],
   "source": [
    "t, v = get_cifar100_with_cls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "db248c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets =[]\n",
    "for i in range(len(t)):\n",
    "    x, y  = t[i]\n",
    "    targets.append(y)\n",
    "targets = np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "cbacea81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cifar_noniid_fix(d, num_users):\n",
    "    \"\"\"\n",
    "    Sample non-I.I.D client data from CIFAR10 dataset\n",
    "    :param dataset:\n",
    "    :param num_users:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    num_shards, num_imgs = 20, 150\n",
    "    idx_shard = [i for i in range(num_shards)]\n",
    "    dict_users = {i: np.array([]) for i in range(num_users)}\n",
    "    idxs = np.arange(num_shards*num_imgs)\n",
    "    # labels = dataset.train_labels.numpy()\n",
    "    \n",
    "    data_y =[]\n",
    "    for i in range(len(d)):\n",
    "        x, y  = d[i]\n",
    "        data_y.append(y)\n",
    "    labels = np.array(data_y)\n",
    "\n",
    "    # sort labels\n",
    "    idxs_labels = np.vstack((idxs, labels))\n",
    "    idxs_labels = idxs_labels[:, idxs_labels[1, :].argsort()]\n",
    "    idxs = idxs_labels[0, :]\n",
    "\n",
    "    # divide and assign\n",
    "    for i in range(num_users):\n",
    "        rand_set = set(np.random.choice(idx_shard, 2, replace=False))\n",
    "        idx_shard = list(set(idx_shard) - rand_set)\n",
    "        for rand in rand_set:\n",
    "            dict_users[i] = np.concatenate(\n",
    "                (dict_users[i], idxs[rand*num_imgs:(rand+1)*num_imgs]), axis=0)\n",
    "    return dict_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "c4c512ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_dict = cifar_noniid_fix(t, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "68293d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n"
     ]
    }
   ],
   "source": [
    "num = 0\n",
    "for k, v in user_dict.items():\n",
    "    num += len(v)\n",
    "print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "8b99df89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform_train = transforms.Compose([\n",
    "        transforms.Pad(4, padding_mode='reflect'),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop(32),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "        np.array([125.3, 123.0, 113.9]) / 255.0,\n",
    "        np.array([63.0, 62.1, 66.7]) / 255.0),])\n",
    "trainset = datasets.CIFAR100(root='../data', train=True, download=True, transform=transform_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "7aca4122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__orig_bases__',\n",
       " '__parameters__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_check_integrity',\n",
       " '_format_transform_repr',\n",
       " '_load_meta',\n",
       " '_repr_indent',\n",
       " 'base_folder',\n",
       " 'class_to_idx',\n",
       " 'classes',\n",
       " 'data',\n",
       " 'download',\n",
       " 'extra_repr',\n",
       " 'filename',\n",
       " 'functions',\n",
       " 'meta',\n",
       " 'register_datapipe_as_function',\n",
       " 'register_function',\n",
       " 'root',\n",
       " 'target_transform',\n",
       " 'targets',\n",
       " 'test_list',\n",
       " 'tgz_md5',\n",
       " 'train',\n",
       " 'train_list',\n",
       " 'transform',\n",
       " 'transforms',\n",
       " 'url']"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "1d11cdc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19,\n",
       " 29,\n",
       " 0,\n",
       " 11,\n",
       " 1,\n",
       " 86,\n",
       " 90,\n",
       " 28,\n",
       " 23,\n",
       " 31,\n",
       " 39,\n",
       " 96,\n",
       " 82,\n",
       " 17,\n",
       " 71,\n",
       " 39,\n",
       " 8,\n",
       " 97,\n",
       " 80,\n",
       " 71,\n",
       " 74,\n",
       " 59,\n",
       " 70,\n",
       " 87,\n",
       " 59,\n",
       " 84,\n",
       " 64,\n",
       " 52,\n",
       " 42,\n",
       " 64,\n",
       " 8,\n",
       " 17,\n",
       " 47,\n",
       " 65,\n",
       " 21,\n",
       " 22,\n",
       " 81,\n",
       " 11,\n",
       " 24,\n",
       " 84,\n",
       " 78,\n",
       " 45,\n",
       " 49,\n",
       " 97,\n",
       " 56,\n",
       " 76,\n",
       " 11,\n",
       " 90,\n",
       " 89,\n",
       " 78,\n",
       " 73,\n",
       " 14,\n",
       " 87,\n",
       " 9,\n",
       " 71,\n",
       " 6,\n",
       " 47,\n",
       " 20,\n",
       " 98,\n",
       " 47,\n",
       " 36,\n",
       " 55,\n",
       " 72,\n",
       " 43,\n",
       " 51,\n",
       " 35,\n",
       " 83,\n",
       " 33,\n",
       " 27,\n",
       " 53,\n",
       " 92,\n",
       " 50,\n",
       " 15,\n",
       " 89,\n",
       " 36,\n",
       " 18,\n",
       " 89,\n",
       " 46,\n",
       " 33,\n",
       " 42,\n",
       " 39,\n",
       " 64,\n",
       " 75,\n",
       " 38,\n",
       " 23,\n",
       " 42,\n",
       " 66,\n",
       " 77,\n",
       " 49,\n",
       " 18,\n",
       " 46,\n",
       " 15,\n",
       " 35,\n",
       " 69,\n",
       " 95,\n",
       " 83,\n",
       " 75,\n",
       " 99,\n",
       " 73,\n",
       " 93,\n",
       " 55,\n",
       " 39,\n",
       " 4,\n",
       " 97,\n",
       " 61,\n",
       " 93,\n",
       " 51,\n",
       " 69,\n",
       " 56,\n",
       " 84,\n",
       " 59,\n",
       " 64,\n",
       " 94,\n",
       " 4,\n",
       " 11,\n",
       " 33,\n",
       " 68,\n",
       " 38,\n",
       " 20,\n",
       " 33,\n",
       " 34,\n",
       " 32,\n",
       " 46,\n",
       " 53,\n",
       " 88,\n",
       " 67,\n",
       " 70,\n",
       " 64,\n",
       " 53,\n",
       " 64,\n",
       " 8,\n",
       " 96,\n",
       " 87,\n",
       " 30,\n",
       " 20,\n",
       " 30,\n",
       " 66,\n",
       " 19,\n",
       " 76,\n",
       " 87,\n",
       " 52,\n",
       " 62,\n",
       " 35,\n",
       " 63,\n",
       " 40,\n",
       " 4,\n",
       " 99,\n",
       " 63,\n",
       " 74,\n",
       " 53,\n",
       " 26,\n",
       " 95,\n",
       " 48,\n",
       " 27,\n",
       " 33,\n",
       " 29,\n",
       " 39,\n",
       " 79,\n",
       " 32,\n",
       " 46,\n",
       " 64,\n",
       " 28,\n",
       " 85,\n",
       " 32,\n",
       " 82,\n",
       " 78,\n",
       " 39,\n",
       " 54,\n",
       " 28,\n",
       " 66,\n",
       " 65,\n",
       " 72,\n",
       " 21,\n",
       " 64,\n",
       " 62,\n",
       " 72,\n",
       " 0,\n",
       " 44,\n",
       " 7,\n",
       " 12,\n",
       " 19,\n",
       " 11,\n",
       " 31,\n",
       " 61,\n",
       " 79,\n",
       " 45,\n",
       " 81,\n",
       " 79,\n",
       " 98,\n",
       " 43,\n",
       " 46,\n",
       " 67,\n",
       " 80,\n",
       " 68,\n",
       " 74,\n",
       " 48,\n",
       " 81,\n",
       " 94,\n",
       " 86,\n",
       " 69,\n",
       " 39,\n",
       " 73,\n",
       " 2,\n",
       " 46,\n",
       " 49,\n",
       " 63,\n",
       " 43,\n",
       " 14,\n",
       " 49,\n",
       " 68,\n",
       " 65,\n",
       " 41,\n",
       " 37,\n",
       " 45,\n",
       " 36,\n",
       " 21,\n",
       " 77,\n",
       " 37,\n",
       " 39,\n",
       " 8,\n",
       " 9,\n",
       " 62,\n",
       " 86,\n",
       " 39,\n",
       " 19,\n",
       " 54,\n",
       " 39,\n",
       " 28,\n",
       " 11,\n",
       " 89,\n",
       " 90,\n",
       " 90,\n",
       " 79,\n",
       " 66,\n",
       " 81,\n",
       " 21,\n",
       " 79,\n",
       " 40,\n",
       " 29,\n",
       " 22,\n",
       " 13,\n",
       " 25,\n",
       " 11,\n",
       " 38,\n",
       " 10,\n",
       " 96,\n",
       " 54,\n",
       " 65,\n",
       " 39,\n",
       " 40,\n",
       " 42,\n",
       " 48,\n",
       " 48,\n",
       " 51,\n",
       " 11,\n",
       " 23,\n",
       " 23,\n",
       " 89,\n",
       " 52,\n",
       " 46,\n",
       " 2,\n",
       " 95,\n",
       " 43,\n",
       " 86,\n",
       " 34,\n",
       " 66,\n",
       " 18,\n",
       " 46,\n",
       " 66,\n",
       " 56,\n",
       " 57,\n",
       " 1,\n",
       " 44,\n",
       " 11,\n",
       " 82,\n",
       " 23,\n",
       " 90,\n",
       " 56,\n",
       " 19,\n",
       " 68,\n",
       " 66,\n",
       " 28,\n",
       " 1,\n",
       " 57,\n",
       " 67,\n",
       " 5,\n",
       " 13,\n",
       " 78,\n",
       " 6,\n",
       " 84,\n",
       " 7,\n",
       " 41,\n",
       " 65,\n",
       " 80,\n",
       " 12,\n",
       " 50,\n",
       " 63,\n",
       " 26,\n",
       " 8,\n",
       " 53,\n",
       " 60,\n",
       " 99,\n",
       " 97,\n",
       " 85,\n",
       " 0,\n",
       " 78,\n",
       " 31,\n",
       " 10,\n",
       " 2,\n",
       " 7,\n",
       " 43,\n",
       " 83,\n",
       " 97,\n",
       " 91,\n",
       " 82,\n",
       " 28,\n",
       " 42,\n",
       " 0,\n",
       " 82,\n",
       " 5,\n",
       " 68,\n",
       " 60,\n",
       " 30,\n",
       " 98,\n",
       " 82,\n",
       " 20,\n",
       " 64,\n",
       " 66,\n",
       " 10,\n",
       " 75,\n",
       " 54,\n",
       " 57,\n",
       " 87,\n",
       " 66,\n",
       " 66,\n",
       " 73,\n",
       " 54,\n",
       " 88,\n",
       " 42,\n",
       " 37,\n",
       " 80,\n",
       " 87,\n",
       " 3,\n",
       " 29,\n",
       " 43,\n",
       " 12,\n",
       " 73,\n",
       " 96,\n",
       " 23,\n",
       " 96,\n",
       " 84,\n",
       " 19,\n",
       " 62,\n",
       " 0,\n",
       " 21,\n",
       " 11,\n",
       " 36,\n",
       " 64,\n",
       " 76,\n",
       " 60,\n",
       " 3,\n",
       " 80,\n",
       " 82,\n",
       " 79,\n",
       " 26,\n",
       " 65,\n",
       " 72,\n",
       " 11,\n",
       " 15,\n",
       " 31,\n",
       " 48,\n",
       " 12,\n",
       " 91,\n",
       " 71,\n",
       " 87,\n",
       " 77,\n",
       " 15,\n",
       " 7,\n",
       " 58,\n",
       " 51,\n",
       " 55,\n",
       " 85,\n",
       " 2,\n",
       " 3,\n",
       " 89,\n",
       " 10,\n",
       " 3,\n",
       " 11,\n",
       " 20,\n",
       " 4,\n",
       " 48,\n",
       " 27,\n",
       " 48,\n",
       " 7,\n",
       " 67,\n",
       " 3,\n",
       " 65,\n",
       " 56,\n",
       " 9,\n",
       " 44,\n",
       " 95,\n",
       " 46,\n",
       " 83,\n",
       " 40,\n",
       " 58,\n",
       " 87,\n",
       " 59,\n",
       " 18,\n",
       " 48,\n",
       " 5,\n",
       " 88,\n",
       " 26,\n",
       " 21,\n",
       " 24,\n",
       " 53,\n",
       " 68,\n",
       " 49,\n",
       " 89,\n",
       " 96,\n",
       " 92,\n",
       " 7,\n",
       " 13,\n",
       " 99,\n",
       " 49,\n",
       " 22,\n",
       " 56,\n",
       " 67,\n",
       " 13,\n",
       " 97,\n",
       " 6,\n",
       " 19,\n",
       " 76,\n",
       " 65,\n",
       " 9,\n",
       " 71,\n",
       " 63,\n",
       " 71,\n",
       " 18,\n",
       " 55,\n",
       " 34,\n",
       " 18,\n",
       " 0,\n",
       " 56,\n",
       " 23,\n",
       " 75,\n",
       " 70,\n",
       " 78,\n",
       " 45,\n",
       " 66,\n",
       " 91,\n",
       " 25,\n",
       " 58,\n",
       " 90,\n",
       " 29,\n",
       " 68,\n",
       " 35,\n",
       " 54,\n",
       " 77,\n",
       " 97,\n",
       " 0,\n",
       " 70,\n",
       " 75,\n",
       " 60,\n",
       " 65,\n",
       " 5,\n",
       " 58,\n",
       " 82,\n",
       " 49,\n",
       " 61,\n",
       " 28,\n",
       " 44,\n",
       " 56,\n",
       " 82,\n",
       " 61,\n",
       " 43,\n",
       " 20,\n",
       " 38,\n",
       " 85,\n",
       " 87,\n",
       " 49,\n",
       " 85,\n",
       " 24,\n",
       " 7,\n",
       " 88,\n",
       " 40,\n",
       " 38,\n",
       " 72,\n",
       " 56,\n",
       " 4,\n",
       " 81,\n",
       " 98,\n",
       " 97,\n",
       " 14,\n",
       " 46,\n",
       " 23,\n",
       " 89,\n",
       " 86,\n",
       " 83,\n",
       " 54,\n",
       " 27,\n",
       " 41,\n",
       " 96,\n",
       " 1,\n",
       " 81,\n",
       " 79,\n",
       " 30,\n",
       " 17,\n",
       " 14,\n",
       " 67,\n",
       " 20,\n",
       " 54,\n",
       " 92,\n",
       " 33,\n",
       " 57,\n",
       " 65,\n",
       " 49,\n",
       " 71,\n",
       " 59,\n",
       " 85,\n",
       " 22,\n",
       " 31,\n",
       " 85,\n",
       " 17,\n",
       " 42,\n",
       " 62,\n",
       " 26,\n",
       " 60,\n",
       " 27,\n",
       " 57,\n",
       " 19,\n",
       " 97,\n",
       " 88,\n",
       " 10,\n",
       " 31,\n",
       " 23,\n",
       " 84,\n",
       " 74,\n",
       " 63,\n",
       " 94,\n",
       " 77,\n",
       " 90,\n",
       " 60,\n",
       " 5,\n",
       " 1,\n",
       " 61,\n",
       " 95,\n",
       " 32,\n",
       " 95,\n",
       " 57,\n",
       " 32,\n",
       " 23,\n",
       " 39,\n",
       " 45,\n",
       " 41,\n",
       " 69,\n",
       " 89,\n",
       " 45,\n",
       " 47,\n",
       " 57,\n",
       " 11,\n",
       " 48,\n",
       " 74,\n",
       " 2,\n",
       " 11,\n",
       " 95,\n",
       " 50,\n",
       " 37,\n",
       " 59,\n",
       " 71,\n",
       " 84,\n",
       " 89,\n",
       " 84,\n",
       " 70,\n",
       " 16,\n",
       " 71,\n",
       " 8,\n",
       " 48,\n",
       " 31,\n",
       " 98,\n",
       " 32,\n",
       " 2,\n",
       " 74,\n",
       " 62,\n",
       " 71,\n",
       " 90,\n",
       " 34,\n",
       " 51,\n",
       " 0,\n",
       " 12,\n",
       " 26,\n",
       " 57,\n",
       " 62,\n",
       " 57,\n",
       " 31,\n",
       " 65,\n",
       " 32,\n",
       " 16,\n",
       " 35,\n",
       " 98,\n",
       " 11,\n",
       " 87,\n",
       " 38,\n",
       " 52,\n",
       " 89,\n",
       " 40,\n",
       " 76,\n",
       " 7,\n",
       " 54,\n",
       " 76,\n",
       " 30,\n",
       " 59,\n",
       " 84,\n",
       " 3,\n",
       " 91,\n",
       " 46,\n",
       " 65,\n",
       " 43,\n",
       " 49,\n",
       " 6,\n",
       " 33,\n",
       " 5,\n",
       " 14,\n",
       " 77,\n",
       " 44,\n",
       " 68,\n",
       " 82,\n",
       " 94,\n",
       " 66,\n",
       " 73,\n",
       " 56,\n",
       " 89,\n",
       " 53,\n",
       " 22,\n",
       " 69,\n",
       " 77,\n",
       " 84,\n",
       " 20,\n",
       " 98,\n",
       " 4,\n",
       " 69,\n",
       " 79,\n",
       " 40,\n",
       " 45,\n",
       " 1,\n",
       " 56,\n",
       " 65,\n",
       " 51,\n",
       " 5,\n",
       " 80,\n",
       " 54,\n",
       " 92,\n",
       " 4,\n",
       " 23,\n",
       " 60,\n",
       " 81,\n",
       " 77,\n",
       " 73,\n",
       " 98,\n",
       " 0,\n",
       " 14,\n",
       " 9,\n",
       " 44,\n",
       " 27,\n",
       " 8,\n",
       " 43,\n",
       " 74,\n",
       " 46,\n",
       " 94,\n",
       " 86,\n",
       " 30,\n",
       " 61,\n",
       " 57,\n",
       " 11,\n",
       " 81,\n",
       " 25,\n",
       " 81,\n",
       " 70,\n",
       " 29,\n",
       " 27,\n",
       " 3,\n",
       " 71,\n",
       " 53,\n",
       " 48,\n",
       " 19,\n",
       " 80,\n",
       " 57,\n",
       " 19,\n",
       " 59,\n",
       " 25,\n",
       " 56,\n",
       " 74,\n",
       " 32,\n",
       " 98,\n",
       " 37,\n",
       " 13,\n",
       " 45,\n",
       " 12,\n",
       " 80,\n",
       " 64,\n",
       " 16,\n",
       " 17,\n",
       " 24,\n",
       " 30,\n",
       " 25,\n",
       " 96,\n",
       " 37,\n",
       " 5,\n",
       " 98,\n",
       " 31,\n",
       " 94,\n",
       " 5,\n",
       " 29,\n",
       " 66,\n",
       " 51,\n",
       " 6,\n",
       " 67,\n",
       " 50,\n",
       " 68,\n",
       " 27,\n",
       " 62,\n",
       " 89,\n",
       " 5,\n",
       " 11,\n",
       " 13,\n",
       " 66,\n",
       " 55,\n",
       " 15,\n",
       " 62,\n",
       " 74,\n",
       " 31,\n",
       " 93,\n",
       " 59,\n",
       " 1,\n",
       " 97,\n",
       " 32,\n",
       " 39,\n",
       " 41,\n",
       " 62,\n",
       " 71,\n",
       " 70,\n",
       " 98,\n",
       " 41,\n",
       " 42,\n",
       " 70,\n",
       " 3,\n",
       " 35,\n",
       " 90,\n",
       " 22,\n",
       " 50,\n",
       " 61,\n",
       " 66,\n",
       " 92,\n",
       " 53,\n",
       " 12,\n",
       " 84,\n",
       " 33,\n",
       " 57,\n",
       " 93,\n",
       " 79,\n",
       " 69,\n",
       " 53,\n",
       " 9,\n",
       " 19,\n",
       " 6,\n",
       " 20,\n",
       " 76,\n",
       " 5,\n",
       " 98,\n",
       " 92,\n",
       " 78,\n",
       " 93,\n",
       " 79,\n",
       " 21,\n",
       " 51,\n",
       " 74,\n",
       " 76,\n",
       " 56,\n",
       " 75,\n",
       " 27,\n",
       " 22,\n",
       " 37,\n",
       " 16,\n",
       " 72,\n",
       " 81,\n",
       " 36,\n",
       " 48,\n",
       " 13,\n",
       " 56,\n",
       " 70,\n",
       " 0,\n",
       " 63,\n",
       " 90,\n",
       " 53,\n",
       " 29,\n",
       " 64,\n",
       " 2,\n",
       " 83,\n",
       " 63,\n",
       " 89,\n",
       " 14,\n",
       " 66,\n",
       " 95,\n",
       " 65,\n",
       " 85,\n",
       " 5,\n",
       " 7,\n",
       " 58,\n",
       " 61,\n",
       " 42,\n",
       " 59,\n",
       " 37,\n",
       " 12,\n",
       " 42,\n",
       " 66,\n",
       " 71,\n",
       " 40,\n",
       " 63,\n",
       " 45,\n",
       " 19,\n",
       " 33,\n",
       " 55,\n",
       " 26,\n",
       " 0,\n",
       " 60,\n",
       " 72,\n",
       " 4,\n",
       " 63,\n",
       " 76,\n",
       " 46,\n",
       " 64,\n",
       " 96,\n",
       " 69,\n",
       " 64,\n",
       " 75,\n",
       " 33,\n",
       " 36,\n",
       " 26,\n",
       " 62,\n",
       " 18,\n",
       " 48,\n",
       " 50,\n",
       " 36,\n",
       " 26,\n",
       " 28,\n",
       " 71,\n",
       " 76,\n",
       " 45,\n",
       " 52,\n",
       " 65,\n",
       " 59,\n",
       " 41,\n",
       " 69,\n",
       " 15,\n",
       " 87,\n",
       " 50,\n",
       " 11,\n",
       " 71,\n",
       " 36,\n",
       " 84,\n",
       " 73,\n",
       " 46,\n",
       " 83,\n",
       " 12,\n",
       " 79,\n",
       " 54,\n",
       " 50,\n",
       " 75,\n",
       " 72,\n",
       " 37,\n",
       " 91,\n",
       " 19,\n",
       " 82,\n",
       " 22,\n",
       " 41,\n",
       " 53,\n",
       " 14,\n",
       " 15,\n",
       " 84,\n",
       " 11,\n",
       " 35,\n",
       " 83,\n",
       " 36,\n",
       " 17,\n",
       " 74,\n",
       " 26,\n",
       " 46,\n",
       " 97,\n",
       " 85,\n",
       " 98,\n",
       " 10,\n",
       " 63,\n",
       " 72,\n",
       " 93,\n",
       " 45,\n",
       " 95,\n",
       " 68,\n",
       " 14,\n",
       " 62,\n",
       " 73,\n",
       " 62,\n",
       " 11,\n",
       " 23,\n",
       " 24,\n",
       " 97,\n",
       " 32,\n",
       " 78,\n",
       " 29,\n",
       " 54,\n",
       " 20,\n",
       " 41,\n",
       " 88,\n",
       " 34,\n",
       " 87,\n",
       " 62,\n",
       " 29,\n",
       " 0,\n",
       " 4,\n",
       " 31,\n",
       " 50,\n",
       " 33,\n",
       " 5,\n",
       " 8,\n",
       " 48,\n",
       " 71,\n",
       " 71,\n",
       " 94,\n",
       " 5,\n",
       " 15,\n",
       " 16,\n",
       " 98,\n",
       " 29,\n",
       " 42,\n",
       " 68,\n",
       " 40,\n",
       " 95,\n",
       " 85,\n",
       " 64,\n",
       " 40,\n",
       " 24,\n",
       " 3,\n",
       " 11,\n",
       " 79,\n",
       " 10,\n",
       " 3,\n",
       " 73,\n",
       " 22,\n",
       " 28,\n",
       " 70,\n",
       " 94,\n",
       " 42,\n",
       " 14,\n",
       " 4,\n",
       " 18,\n",
       " 86,\n",
       " 4,\n",
       " 32,\n",
       " 38,\n",
       " 24,\n",
       " 65,\n",
       " 75,\n",
       " 1,\n",
       " 7,\n",
       " 99,\n",
       " 58,\n",
       " 8,\n",
       " 67,\n",
       " 10,\n",
       " 44,\n",
       " 98,\n",
       " 88,\n",
       " 31,\n",
       " 83,\n",
       " 63,\n",
       " 59,\n",
       " 34,\n",
       " 81,\n",
       " 54,\n",
       " 28,\n",
       " 76,\n",
       " 84,\n",
       " 44,\n",
       " 88,\n",
       " 59,\n",
       " 65,\n",
       " 8,\n",
       " 91,\n",
       " 51,\n",
       " 42,\n",
       " 23,\n",
       " 20,\n",
       " 26,\n",
       " 77,\n",
       " 77,\n",
       " 4,\n",
       " 84,\n",
       " 4,\n",
       " 8,\n",
       " 60,\n",
       " 41,\n",
       " 96,\n",
       " ...]"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "90195579",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR100Coarse(datasets.CIFAR100):\n",
    "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False, superclass=False):\n",
    "        super(CIFAR100Coarse, self).__init__(root, train, transform, target_transform, download)\n",
    "        if superclass:\n",
    "        # update labels\n",
    "            coarse_labels = np.array([ 4,  1, 14,  8,  0,  6,  7,  7, 18,  3,\n",
    "                                   3, 14,  9, 18,  7, 11,  3,  9,  7, 11,\n",
    "                                   6, 11,  5, 10,  7,  6, 13, 15,  3, 15, \n",
    "                                   0, 11,  1, 10, 12, 14, 16,  9, 11,  5,\n",
    "                                   5, 19,  8,  8, 15, 13, 14, 17, 18, 10,\n",
    "                                   16, 4, 17,  4,  2,  0, 17,  4, 18, 17,\n",
    "                                   10, 3,  2, 12, 12, 16, 12,  1,  9, 19, \n",
    "                                   2, 10,  0,  1, 16, 12,  9, 13, 15, 13,\n",
    "                                  16, 19,  2,  4,  6, 19,  5,  5,  8, 19,\n",
    "                                  18,  1,  2, 15,  6,  0, 17,  8, 14, 13])\n",
    "            self.targets = coarse_labels[self.targets]\n",
    "            self.targets = self.targets.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "a6944d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform_train = transforms.Compose([\n",
    "        transforms.Pad(4, padding_mode='reflect'),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop(32),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "        np.array([125.3, 123.0, 113.9]) / 255.0,\n",
    "        np.array([63.0, 62.1, 66.7]) / 255.0),])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "        np.array([125.3, 123.0, 113.9]) / 255.0,\n",
    "        np.array([63.0, 62.1, 66.7]) / 255.0),])\n",
    "supercls_train = CIFAR100Coarse(root='../data', train=True, download=True, transform=transform_train, superclass=True)\n",
    "subcls_train = CIFAR100Coarse(root='../data', train=True, download=True, transform=transform_train, superclass=False)\n",
    "supercls_test = CIFAR100Coarse(root='../data', train=False, download=True, transform=transform_test, superclass=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "789be02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "relations = [set() for i in range(20)]\n",
    "for i, (x, y_fine) in enumerate(subcls_train):\n",
    "    relations[supercls_train[i][1]].add(y_fine)\n",
    "for i in range(len(relations)):\n",
    "    relations[i] = list(relations[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "3b3a6863",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 72, 55, 30, 95]\n",
      "[32, 1, 67, 73, 91]\n",
      "[70, 82, 54, 92, 62]\n",
      "[9, 10, 16, 28, 61]\n",
      "[0, 83, 51, 53, 57]\n",
      "[39, 40, 22, 87, 86]\n",
      "[5, 84, 20, 25, 94]\n",
      "[6, 7, 14, 18, 24]\n",
      "[97, 3, 42, 43, 88]\n",
      "[68, 37, 76, 12, 17]\n",
      "[33, 71, 49, 23, 60]\n",
      "[38, 15, 19, 21, 31]\n",
      "[64, 66, 34, 75, 63]\n",
      "[99, 77, 45, 79, 26]\n",
      "[2, 35, 98, 11, 46]\n",
      "[44, 78, 29, 27, 93]\n",
      "[65, 36, 74, 80, 50]\n",
      "[96, 47, 52, 56, 59]\n",
      "[58, 8, 13, 48, 90]\n",
      "[69, 41, 81, 85, 89]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(relations)):\n",
    "    print(relations[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "dcfa7e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "private_classes =[0,1, 7, 9, 12,18]#超类\n",
    "fine_classes_in_use = [[relations[j][i%5] for j in private_classes] for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "635a64f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 32, 6, 68, 64, 58],\n",
       " [72, 1, 7, 37, 66, 8],\n",
       " [55, 67, 14, 76, 34, 13],\n",
       " [30, 73, 18, 12, 75, 48],\n",
       " [95, 91, 24, 17, 63, 90],\n",
       " [4, 32, 6, 68, 64, 58],\n",
       " [72, 1, 7, 37, 66, 8],\n",
       " [55, 67, 14, 76, 34, 13],\n",
       " [30, 73, 18, 12, 75, 48],\n",
       " [95, 91, 24, 17, 63, 90]]"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_classes_in_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "fcc9153c",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_dict = {i: [] for i in range(10)}\n",
    "subcls_train_labels = torch.Tensor(subcls_train.targets)\n",
    "for index, cls_list in enumerate(fine_classes_in_use):\n",
    "    t_indices = []\n",
    "    for i in cls_list:\n",
    "        t_indices += random.sample((subcls_train_labels == i).nonzero().view(-1).tolist(), 20)\n",
    "    user_dict[index] += t_indices\n",
    "\n",
    "v_indices = []\n",
    "supercls_test_labels = torch.Tensor(supercls_test.targets)\n",
    "for cls in private_classes:\n",
    "    v_indices += random.sample((supercls_test_labels == cls).nonzero().view(-1).tolist(), 500)\n",
    "testsubset = Subset(supercls_test, v_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "9853bb8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 72, 55, 30, 95],\n",
       " [32, 1, 67, 73, 91],\n",
       " [70, 82, 54, 92, 62],\n",
       " [9, 10, 16, 28, 61],\n",
       " [0, 83, 51, 53, 57],\n",
       " [39, 40, 22, 87, 86],\n",
       " [5, 84, 20, 25, 94],\n",
       " [6, 7, 14, 18, 24],\n",
       " [97, 3, 42, 43, 88],\n",
       " [68, 37, 76, 12, 17],\n",
       " [33, 71, 49, 23, 60],\n",
       " [38, 15, 19, 21, 31],\n",
       " [64, 66, 34, 75, 63],\n",
       " [99, 77, 45, 79, 26],\n",
       " [2, 35, 98, 11, 46],\n",
       " [44, 78, 29, 27, 93],\n",
       " [65, 36, 74, 80, 50],\n",
       " [96, 47, 52, 56, 59],\n",
       " [58, 8, 13, 48, 90],\n",
       " [69, 41, 81, 85, 89]]"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "57514ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR100Coarse(datasets.CIFAR100):\n",
    "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False, superclass=False):\n",
    "        super(CIFAR100Coarse, self).__init__(root, train, transform, target_transform, download)\n",
    "        if superclass:\n",
    "        # update labels\n",
    "            coarse_labels = np.array([ 4,  1, 14,  8,  0,  6,  7,  7, 18,  3,\n",
    "                                   3, 14,  9, 18,  7, 11,  3,  9,  7, 11,\n",
    "                                   6, 11,  5, 10,  7,  6, 13, 15,  3, 15, \n",
    "                                   0, 11,  1, 10, 12, 14, 16,  9, 11,  5,\n",
    "                                   5, 19,  8,  8, 15, 13, 14, 17, 18, 10,\n",
    "                                   16, 4, 17,  4,  2,  0, 17,  4, 18, 17,\n",
    "                                   10, 3,  2, 12, 12, 16, 12,  1,  9, 19, \n",
    "                                   2, 10,  0,  1, 16, 12,  9, 13, 15, 13,\n",
    "                                  16, 19,  2,  4,  6, 19,  5,  5,  8, 19,\n",
    "                                  18,  1,  2, 15,  6,  0, 17,  8, 14, 13])\n",
    "            self.targets = coarse_labels[self.targets]\n",
    "            self.targets = self.targets.tolist()\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "fine_classes_in_use = [[4, 32, 6, 68, 64, 58],\n",
    " [72, 1, 7, 37, 66, 8],\n",
    " [55, 67, 14, 76, 34, 13],\n",
    " [30, 73, 18, 12, 75, 48],\n",
    " [95, 91, 24, 17, 63, 90],\n",
    " [4, 32, 6, 68, 64, 58],\n",
    " [72, 1, 7, 37, 66, 8],\n",
    " [55, 67, 14, 76, 34, 13],\n",
    " [30, 73, 18, 12, 75, 48],\n",
    " [95, 91, 24, 17, 63, 90]]\n",
    "\n",
    "\n",
    "relations = [[4, 72, 55, 30, 95],\n",
    " [32, 1, 67, 73, 91],\n",
    " [70, 82, 54, 92, 62],\n",
    " [9, 10, 16, 28, 61],\n",
    " [0, 83, 51, 53, 57],\n",
    " [39, 40, 22, 87, 86],\n",
    " [5, 84, 20, 25, 94],\n",
    " [6, 7, 14, 18, 24],\n",
    " [97, 3, 42, 43, 88],\n",
    " [68, 37, 76, 12, 17],\n",
    " [33, 71, 49, 23, 60],\n",
    " [38, 15, 19, 21, 31],\n",
    " [64, 66, 34, 75, 63],\n",
    " [99, 77, 45, 79, 26],\n",
    " [2, 35, 98, 11, 46],\n",
    " [44, 78, 29, 27, 93],\n",
    " [65, 36, 74, 80, 50],\n",
    " [96, 47, 52, 56, 59],\n",
    " [58, 8, 13, 48, 90],\n",
    " [69, 41, 81, 85, 89]]\n",
    "\n",
    "private_classes =[0,1, 7, 9, 12,18]#超类\n",
    "\n",
    "def getcifar100_with_supercls(N_parties=10):\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.Pad(4, padding_mode='reflect'),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop(32),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "        np.array([125.3, 123.0, 113.9]) / 255.0,\n",
    "        np.array([63.0, 62.1, 66.7]) / 255.0),])\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "        np.array([125.3, 123.0, 113.9]) / 255.0,\n",
    "        np.array([63.0, 62.1, 66.7]) / 255.0),])\n",
    "    supercls_train = CIFAR100Coarse(root='../data', train=True, download=True, transform=transform_train, superclass=True)\n",
    "    subcls_train = CIFAR100Coarse(root='../data', train=True, download=True, transform=transform_train, superclass=False)\n",
    "    supercls_test = CIFAR100Coarse(root='../data', train=False, download=True, transform=transform_test, superclass=True)\n",
    "    \n",
    "    user_dict = {i: [] for i in range(N_parties)}\n",
    "    subcls_train_labels = torch.Tensor(subcls_train.targets)\n",
    "    for index, cls_list in enumerate(fine_classes_in_use):\n",
    "        t_indices = []\n",
    "        for i in cls_list:\n",
    "            t_indices += random.sample((subcls_train_labels == i).nonzero().view(-1).tolist(), 20)\n",
    "        user_dict[index] += t_indices\n",
    "\n",
    "    v_indices = []\n",
    "    supercls_test_labels = torch.Tensor(supercls_test.targets)\n",
    "    for cls in private_classes:\n",
    "        v_indices += random.sample((supercls_test_labels == cls).nonzero().view(-1).tolist(), 500)\n",
    "    testsubset = Subset(supercls_test, v_indices)\n",
    "    return supercls_train, testsubset, user_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "9790bc25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "finaltrainset, finaltestset, user_groups = getcifar100_with_supercls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "66b6affa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1308, 21103, 34225, 33850, 30378, 48926, 4899, 43786, 49250, 23909]\n",
      "[31944, 38128, 25983, 27464, 29205, 23207, 25901, 45978, 8629, 11044]\n",
      "[33296, 24604, 10728, 8437, 13796, 2082, 32703, 14770, 26516, 8655]\n",
      "[37692, 34802, 32745, 41012, 49185, 29594, 47038, 18472, 24298, 9760]\n",
      "[36539, 44045, 31116, 20410, 38447, 1046, 30975, 46459, 19214, 37254]\n",
      "[11182, 651, 11985, 19118, 16923, 23853, 48414, 49186, 35006, 5677]\n",
      "[23232, 28031, 28976, 29205, 49495, 47808, 15010, 19614, 30455, 41205]\n",
      "[2239, 41674, 10217, 10728, 8655, 380, 17989, 18343, 23475, 34227]\n",
      "[37692, 1019, 40166, 25100, 18446, 2569, 18447, 16634, 47155, 24104]\n",
      "[31227, 14203, 4938, 16062, 20410, 9428, 46855, 10916, 5054, 8658]\n"
     ]
    }
   ],
   "source": [
    "for k, v in user_groups.items():\n",
    "    print(v[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "6a826f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "a = torch.rand(4,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "1e9318db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1315, 0.3357, 0.8177, 0.3827, 0.4246, 0.5357],\n",
       "        [0.9639, 0.2344, 0.2742, 0.2517, 0.9858, 0.1735],\n",
       "        [0.5536, 0.1186, 0.7834, 0.9057, 0.5682, 0.9217],\n",
       "        [0.4434, 0.5014, 0.4769, 0.8707, 0.9203, 0.3532]])"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "bef7374d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5, 2, 4],\n",
      "        [0, 4, 2],\n",
      "        [3, 5, 2],\n",
      "        [4, 3, 1]])\n",
      "tensor([[0.5357, 0.8177, 0.4246],\n",
      "        [0.9639, 0.9858, 0.2742],\n",
      "        [0.9057, 0.9217, 0.7834],\n",
      "        [0.9203, 0.8707, 0.5014]])\n",
      "tensor([[0.2382],\n",
      "        [0.2587],\n",
      "        [0.2129],\n",
      "        [0.2253]])\n",
      "tensor([[2],\n",
      "        [4],\n",
      "        [5],\n",
      "        [4]])\n",
      "tensor([[0.5035, 0.6176, 1.0000, 0.6473, 0.6749, 0.7543],\n",
      "        [0.9784, 0.4717, 0.4909, 0.4800, 1.0000, 0.4439],\n",
      "        [0.6921, 0.4479, 0.8709, 0.9842, 0.7022, 1.0000],\n",
      "        [0.6207, 0.6577, 0.6418, 0.9516, 1.0000, 0.5671]])\n"
     ]
    }
   ],
   "source": [
    "a_f = F.softmax(a, dim=1)\n",
    "val, ind = a.topk(3, 1, largest=True, sorted=False)\n",
    "val2, ind2 = a_f.topk(1, 1, largest=True, sorted=False)\n",
    "w = a_f / val2\n",
    "print(ind)\n",
    "print(val)\n",
    "print(val2)\n",
    "print(ind2)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "4be6b540",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_onehot = F.one_hot(ind, 6)\n",
    "multi_label = torch.sum(label_onehot, 1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "df8c5949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 1., 0., 1., 1.],\n",
       "        [1., 0., 1., 0., 1., 0.],\n",
       "        [0., 0., 1., 1., 0., 1.],\n",
       "        [0., 1., 0., 1., 1., 0.]])"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "a4461dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_m_l = multi_label * w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "886d52e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 1.0000, 0.0000, 0.6749, 0.7543],\n",
       "        [0.9784, 0.0000, 0.4909, 0.0000, 1.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.8709, 0.9842, 0.0000, 1.0000],\n",
       "        [0.0000, 0.6577, 0.0000, 0.9516, 1.0000, 0.0000]])"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_m_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "0a38b443",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (6) must match the size of tensor b (3) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24381/1082086500.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlogit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw_m_l\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (6) must match the size of tensor b (3) at non-singleton dimension 1"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e7fcfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
